{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee645fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des bibiotheque necessaire\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "import re\n",
    "#from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import fr_core_news_md #import spacy french stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e3fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "def tokenize_text(corpus):\n",
    "    tokensCorpus=[]\n",
    "    for doc in corpus:\n",
    "        doc_tokens = word_tokenize(doc)\n",
    "        tokensCorpus.append(doc_tokens)\n",
    "    return tokensCorpus\n",
    "\n",
    "# removing stopwords\n",
    "def remove_stopwords(corpus):\n",
    "    filtered_corpus=[]\n",
    "    for tokens in corpus:\n",
    "        #french_sw = stopwords.words('french') \n",
    "        french_sw=list(STOP_WORDS) #get french stopwords\n",
    "        filtered_tokens = [token for token in tokens.split() if token not in french_sw and len(token)>2]\n",
    "        filtred_text=' '.join(filtered_tokens) #reforme le text du documents separé par espace\n",
    "        filtered_corpus.append(filtred_text)\n",
    "    return filtered_corpus\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    list_bigrams=[]\n",
    "    for doc in texts :\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(doc)\n",
    "        # bigrams \n",
    "        bigrams = [w for w in ngrams(tokens,n=2)]\n",
    "        list_bigrams.append(bigrams)\n",
    "    return list_bigrams\n",
    "\n",
    "def unique_bigrams(texts):\n",
    "    list_bigrams = make_bigrams(texts)\n",
    "    list_unique_bigrams=[]\n",
    "    for list_bigram in list_bigrams :\n",
    "        list_unique_bigrams.append(list(set(list_bigram)))\n",
    "    return list_unique_bigrams\n",
    "\n",
    "def count_unique_bigrams(texts):\n",
    "    list_unique_bigrams = unique_bigrams(texts)\n",
    "    list_count_unique_bigrams = []\n",
    "    for list_unique_bigram in list_unique_bigrams :\n",
    "        list_count_unique_bigrams.append(len(list_unique_bigram))\n",
    "    return list_count_unique_bigrams\n",
    "\n",
    "def tf_function(text,word):\n",
    "    n=len(word_tokenize(text)) #nombre de mots dans le texte\n",
    "    m= word_tokenize(text).count(word) # nombre d'occurence du mot en parametre dans le texte\n",
    "    return  m/n\n",
    "\n",
    "def idf_function(texts,word):\n",
    "    list_idf = []\n",
    "    s = 0\n",
    "    for text in  texts :\n",
    "        if word in word_tokenize(text):\n",
    "            s = s + 1\n",
    "    return np.log((len(texts)+1)/(s+1))+1\n",
    "\n",
    "def tf_idf_function(texts,text,word):\n",
    "    return tf_function(text,word) * idf_function(texts,word)\n",
    "\n",
    "#output French accents correctly\n",
    "def convert_accents(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore')\n",
    "\n",
    "#convertisse les documents en minuscule\n",
    "def lower_text(corpus):\n",
    "    LowerCorpus=[]\n",
    "    for doc in corpus:\n",
    "        lowerDoc=str(doc).lower() #convertissent le texte en minuscules\n",
    "        lowerDoc=convert_accents(lowerDoc).decode(\"utf-8\") #supprimes les accents\n",
    "        LowerCorpus.append(lowerDoc)\n",
    "    return LowerCorpus\n",
    "\n",
    "#supprimes caracteres speciaux\n",
    "def remove_characters(corpus,keep_apostrophes=True):\n",
    "    filtered_corpus=[]\n",
    "    for doc in corpus:\n",
    "        doc = doc.strip()\n",
    "        if keep_apostrophes:\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            doc =re.sub(\"l\\'\",\"\",doc)\n",
    "            doc =re.sub(\"d\\'\",\"\",doc)\n",
    "            PATTERN = r'[?|$|&|*|-|!|%|@|(|)|~|\\d]'\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "            filtered_corpus.append(filtered_doc)\n",
    "        else:\n",
    "            PATTERN = r'[^a-zA-Z ]'\n",
    "            #supprimes les urls\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "        \n",
    "            filtered_corpus.append(filtered_doc)\n",
    "    return filtered_corpus\n",
    "\n",
    "#recuperer les mots qui apparaissent dans plusieurs documents\n",
    "def get_mostCommonWords(corpus,max_freq=100):\n",
    "    vocabulaire=dict() #dictionnaire qui va contenir le nombre d'occurence des mots dans les documents\n",
    "    for doc in corpus:\n",
    "        for word in set(doc.split()): #recupere les mots unique de chaque documents\n",
    "            if word in vocabulaire:\n",
    "                vocabulaire[word]+=1\n",
    "            else:\n",
    "                vocabulaire[word]=1\n",
    "    \n",
    "    #recupere les dont le nombre d'occurences dans les documents > max_freq\n",
    "    mostCommonsWord=[word for word,value in vocabulaire.items() if value>max_freq ]\n",
    "        \n",
    "    return mostCommonsWord\n",
    "\n",
    "#lemmatisation\n",
    "def lemm_tokens(corpus):\n",
    "    \n",
    "    nlp = fr_core_news_md.load() #initialisation du model \"fr_core_news_md\" de spacy\n",
    "    allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    corpus_lemms=[]\n",
    "    \n",
    "    for document in corpus:\n",
    "        doc = nlp(document)\n",
    "        lemms=[token.lemma_ for token in doc if token.pos_ in allowed_postags] #recupere les lemms des tokens\n",
    "        text=' '.join(lemms) #reforme le text du documents separé par espace\n",
    "        corpus_lemms.append(text)\n",
    "            \n",
    "    return corpus_lemms\n",
    "#fonction qui supprimes les documents vides ou tres courte\n",
    "def remove_shortDocument(corpus,min_length=3):\n",
    "    filtred_corpus=[]\n",
    "    idx_doc=[]\n",
    "    for idx,doc in enumerate(corpus):\n",
    "        \n",
    "        if len(doc.split())>min_length:\n",
    "            filtred_corpus.append(doc)\n",
    "            idx_doc.append(idx)\n",
    "        \n",
    "    \n",
    "    return filtred_corpus,idx_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c806910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    \n",
    "    corpus=remove_characters(corpus)\n",
    "    corpus=lower_text(corpus)\n",
    "    corpus=remove_stopwords(corpus)\n",
    "    corpus=lemm_tokens(corpus)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "145a7873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Confédération générale des entreprises du M...</td>\n",
       "      <td>2022-06-09</td>\n",
       "      <td>https://www.challenge.ma/maroc-emirats-arabes-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selon Bank Al-Maghrib, l’encours du crédit ban...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/banques-les-prets-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dans le cadre des Rencontres du Livre Blanc, l...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/la-cgem-met-le-cap-su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pour la première fois depuis la création des I...</td>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>https://www.challenge.ma/le-maroc-pays-africai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les levées des capitaux se sont établies à plu...</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>https://www.challenge.ma/marche-des-capitaux-l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        date  \\\n",
       "0  La Confédération générale des entreprises du M...  2022-06-09   \n",
       "1  Selon Bank Al-Maghrib, l’encours du crédit ban...  2022-06-03   \n",
       "2  Dans le cadre des Rencontres du Livre Blanc, l...  2022-06-03   \n",
       "3  Pour la première fois depuis la création des I...  2022-06-02   \n",
       "4  Les levées des capitaux se sont établies à plu...  2022-06-01   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.challenge.ma/maroc-emirats-arabes-...  \n",
       "1  https://www.challenge.ma/banques-les-prets-acc...  \n",
       "2  https://www.challenge.ma/la-cgem-met-le-cap-su...  \n",
       "3  https://www.challenge.ma/le-maroc-pays-africai...  \n",
       "4  https://www.challenge.ma/marche-des-capitaux-l...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(r\"..\\Scraping\\Raw_data\\1.csv\", sep=\",\")\n",
    "df2 = pd.read_csv(r\"..\\Scraping\\Raw_data\\2.csv\", sep=\",\")\n",
    "df3 = pd.read_csv(r\"..\\Scraping\\Raw_data\\3.csv\", sep=\",\")\n",
    "dataset = pd.concat([df1, df2, df3])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5314702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Confédération générale des entreprises du M...</td>\n",
       "      <td>2022-06-09</td>\n",
       "      <td>https://www.challenge.ma/maroc-emirats-arabes-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selon Bank Al-Maghrib, l’encours du crédit ban...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/banques-les-prets-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dans le cadre des Rencontres du Livre Blanc, l...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/la-cgem-met-le-cap-su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pour la première fois depuis la création des I...</td>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>https://www.challenge.ma/le-maroc-pays-africai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les levées des capitaux se sont établies à plu...</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>https://www.challenge.ma/marche-des-capitaux-l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        date  \\\n",
       "0  La Confédération générale des entreprises du M...  2022-06-09   \n",
       "1  Selon Bank Al-Maghrib, l’encours du crédit ban...  2022-06-03   \n",
       "2  Dans le cadre des Rencontres du Livre Blanc, l...  2022-06-03   \n",
       "3  Pour la première fois depuis la création des I...  2022-06-02   \n",
       "4  Les levées des capitaux se sont établies à plu...  2022-06-01   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.challenge.ma/maroc-emirats-arabes-...  \n",
       "1  https://www.challenge.ma/banques-les-prets-acc...  \n",
       "2  https://www.challenge.ma/la-cgem-met-le-cap-su...  \n",
       "3  https://www.challenge.ma/le-maroc-pays-africai...  \n",
       "4  https://www.challenge.ma/marche-des-capitaux-l...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26c4b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du corpus = 74 Documents\n"
     ]
    }
   ],
   "source": [
    "corpus = dataset.text.values.tolist()\n",
    "print(\"Taille du corpus = \"+str(len(corpus))+\" Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24aca84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocessing(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff190cd",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "178a2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build TFIDF features on train reviews with a specifique vocabulary\n",
    "corpus_lemmatized=tokenize_text(corpus) \n",
    "id2word = corpora.Dictionary(corpus_lemmatized)\n",
    "vocabulaire=id2word.token2id #get vocabulary dict where keys are terms and values are indices in the feature matrix\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0,sublinear_tf=True,lowercase=True,ngram_range=(1,2),vocabulary=vocabulaire)\n",
    "tfidf_train_features = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90ae8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1133b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "tfidf_train_features_array = tfidf_train_features.toarray()\n",
    "tf_idf_frame = pd.DataFrame(tfidf_train_features_array,columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efa68419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-</th>\n",
       "      <th>abdelmajid</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>actuel</th>\n",
       "      <th>affaire</th>\n",
       "      <th>affirmer</th>\n",
       "      <th>ajoutee</th>\n",
       "      <th>ajouter</th>\n",
       "      <th>albare</th>\n",
       "      <th>algerienn</th>\n",
       "      <th>...</th>\n",
       "      <th>migrate</th>\n",
       "      <th>outsourcia</th>\n",
       "      <th>reagissant</th>\n",
       "      <th>relais</th>\n",
       "      <th>sp</th>\n",
       "      <th>spe</th>\n",
       "      <th>sujette</th>\n",
       "      <th>suspensif</th>\n",
       "      <th>trajectoire</th>\n",
       "      <th>usuel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068419</td>\n",
       "      <td>0.115843</td>\n",
       "      <td>0.039628</td>\n",
       "      <td>0.052164</td>\n",
       "      <td>0.054862</td>\n",
       "      <td>0.046165</td>\n",
       "      <td>0.041909</td>\n",
       "      <td>0.068419</td>\n",
       "      <td>0.068419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     -  abdelmajid  abdullah    actuel   affaire  affirmer   ajoutee  \\\n",
       "0  0.0    0.068419  0.115843  0.039628  0.052164  0.054862  0.046165   \n",
       "1  0.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    ajouter    albare  algerienn  ...  migrate  outsourcia  reagissant  \\\n",
       "0  0.041909  0.068419   0.068419  ...      0.0         0.0         0.0   \n",
       "1  0.000000  0.000000   0.000000  ...      0.0         0.0         0.0   \n",
       "2  0.000000  0.000000   0.000000  ...      0.0         0.0         0.0   \n",
       "3  0.000000  0.000000   0.000000  ...      0.0         0.0         0.0   \n",
       "4  0.000000  0.000000   0.000000  ...      0.0         0.0         0.0   \n",
       "\n",
       "   relais   sp  spe  sujette  suspensif  trajectoire  usuel  \n",
       "0     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "1     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "2     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "3     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "4     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "\n",
       "[5 rows x 4753 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3cfdf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tfidf_train_features_array to csv file\n",
    "tf_idf_frame.to_csv(r\"..\\Scraping\\Raw_data\\tf_idf_train_features_array.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc880138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "19084cb2a221719542bd94e588e69917ee95725b301fa5f2df470f319dde6836"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
