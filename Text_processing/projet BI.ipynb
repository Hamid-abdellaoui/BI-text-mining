{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee645fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des bibiotheque necessaire\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "import re\n",
    "#from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import fr_core_news_md #import spacy french stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12e3fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "def tokenize_text(corpus):\n",
    "    tokensCorpus=[]\n",
    "    for doc in corpus:\n",
    "        doc_tokens = word_tokenize(doc)\n",
    "        tokensCorpus.append(doc_tokens)\n",
    "    return tokensCorpus\n",
    "\n",
    "# removing stopwords\n",
    "def remove_stopwords(corpus):\n",
    "    filtered_corpus=[]\n",
    "    for tokens in corpus:\n",
    "        #french_sw = stopwords.words('french') \n",
    "        french_sw=list(STOP_WORDS) #get french stopwords\n",
    "        filtered_tokens = [token for token in tokens.split() if token not in french_sw and len(token)>2]\n",
    "        filtred_text=' '.join(filtered_tokens) #reforme le text du documents separé par espace\n",
    "        filtered_corpus.append(filtred_text)\n",
    "    return filtered_corpus\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    list_bigrams=[]\n",
    "    for doc in texts :\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(doc)\n",
    "        # bigrams \n",
    "        bigrams = [w for w in ngrams(tokens,n=2)]\n",
    "        list_bigrams.append(bigrams)\n",
    "    return list_bigrams\n",
    "\n",
    "def unique_bigrams(texts):\n",
    "    list_bigrams = make_bigrams(texts)\n",
    "    list_unique_bigrams=[]\n",
    "    for list_bigram in list_bigrams :\n",
    "        list_unique_bigrams.append(list(set(list_bigram)))\n",
    "    return list_unique_bigrams\n",
    "\n",
    "def count_unique_bigrams(texts):\n",
    "    list_unique_bigrams = unique_bigrams(texts)\n",
    "    list_count_unique_bigrams = []\n",
    "    for list_unique_bigram in list_unique_bigrams :\n",
    "        list_count_unique_bigrams.append(len(list_unique_bigram))\n",
    "    return list_count_unique_bigrams\n",
    "\n",
    "def tf_function(bigrams,bigram):\n",
    "    n=len(bigrams) #nombre de bigrams uniques dans le texte\n",
    "    m= bigrams.count(bigram) # nombre d'occurence du bigram en parametre dans la liste de bigrams\n",
    "    return  m/(n+1)\n",
    "\n",
    "def idf_function(whole_bigram,bigram):\n",
    "    list_idf = []\n",
    "    s = 0\n",
    "    for bigrams in whole_bigram:  \n",
    "        if bigram in bigrams:\n",
    "            s += 1\n",
    "    return np.log((len(whole_bigram)/(s+1))+1)\n",
    "\n",
    "def tf_idf_function(whole_bigram,bigrams,bigram):\n",
    "    return tf_function(bigrams,bigram) * idf_function(whole_bigram,bigram)\n",
    "\n",
    "#output French accents correctly\n",
    "def convert_accents(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore')\n",
    "\n",
    "#convertisse les documents en minuscule\n",
    "def lower_text(corpus):\n",
    "    LowerCorpus=[]\n",
    "    for doc in corpus:\n",
    "        lowerDoc=str(doc).lower() #convertissent le texte en minuscules\n",
    "        lowerDoc=convert_accents(lowerDoc).decode(\"utf-8\") #supprimes les accents\n",
    "        LowerCorpus.append(lowerDoc)\n",
    "    return LowerCorpus\n",
    "\n",
    "#supprimes caracteres speciaux\n",
    "def remove_characters(corpus,keep_apostrophes=True):\n",
    "    filtered_corpus=[]\n",
    "    for doc in corpus:\n",
    "        doc = doc.strip()\n",
    "        if keep_apostrophes:\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            doc =re.sub(\"l\\'\",\"\",doc)\n",
    "            doc =re.sub(\"d\\'\",\"\",doc)\n",
    "            PATTERN = r'[?|$|&|*|-|!|%|@|(|)|~|\\d]'\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "            filtered_corpus.append(filtered_doc)\n",
    "        else:\n",
    "            PATTERN = r'[^a-zA-Z ]'\n",
    "            #supprimes les urls\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "        \n",
    "            filtered_corpus.append(filtered_doc)\n",
    "    return filtered_corpus\n",
    "\n",
    "#recuperer les mots qui apparaissent dans plusieurs documents\n",
    "def get_mostCommonWords(corpus,max_freq=100):\n",
    "    vocabulaire=dict() #dictionnaire qui va contenir le nombre d'occurence des mots dans les documents\n",
    "    for doc in corpus:\n",
    "        for word in set(doc.split()): #recupere les mots unique de chaque documents\n",
    "            if word in vocabulaire:\n",
    "                vocabulaire[word]+=1\n",
    "            else:\n",
    "                vocabulaire[word]=1\n",
    "    \n",
    "    #recupere les dont le nombre d'occurences dans les documents > max_freq\n",
    "    mostCommonsWord=[word for word,value in vocabulaire.items() if value>max_freq ]\n",
    "        \n",
    "    return mostCommonsWord\n",
    "\n",
    "#lemmatisation\n",
    "def lemm_tokens(corpus):\n",
    "    \n",
    "    nlp = fr_core_news_md.load() #initialisation du model \"fr_core_news_md\" de spacy\n",
    "    allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    corpus_lemms=[]\n",
    "    \n",
    "    for document in corpus:\n",
    "        doc = nlp(document)\n",
    "        lemms=[token.lemma_ for token in doc if token.pos_ in allowed_postags] #recupere les lemms des tokens\n",
    "        text=' '.join(lemms) #reforme le text du documents separé par espace\n",
    "        corpus_lemms.append(text)\n",
    "            \n",
    "    return corpus_lemms\n",
    "#fonction qui supprimes les documents vides ou tres courte\n",
    "def remove_shortDocument(corpus,min_length=3):\n",
    "    filtred_corpus=[]\n",
    "    idx_doc=[]\n",
    "    for idx,doc in enumerate(corpus):\n",
    "        \n",
    "        if len(doc.split())>min_length:\n",
    "            filtred_corpus.append(doc)\n",
    "            idx_doc.append(idx)\n",
    "        \n",
    "    \n",
    "    return filtred_corpus,idx_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c806910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    \n",
    "    corpus=remove_characters(corpus)\n",
    "    corpus=lower_text(corpus)\n",
    "    corpus=remove_stopwords(corpus)\n",
    "    corpus=lemm_tokens(corpus)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145a7873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Confédération générale des entreprises du M...</td>\n",
       "      <td>2022-06-09</td>\n",
       "      <td>https://www.challenge.ma/maroc-emirats-arabes-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selon Bank Al-Maghrib, l’encours du crédit ban...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/banques-les-prets-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dans le cadre des Rencontres du Livre Blanc, l...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/la-cgem-met-le-cap-su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pour la première fois depuis la création des I...</td>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>https://www.challenge.ma/le-maroc-pays-africai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les levées des capitaux se sont établies à plu...</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>https://www.challenge.ma/marche-des-capitaux-l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        date  \\\n",
       "0  La Confédération générale des entreprises du M...  2022-06-09   \n",
       "1  Selon Bank Al-Maghrib, l’encours du crédit ban...  2022-06-03   \n",
       "2  Dans le cadre des Rencontres du Livre Blanc, l...  2022-06-03   \n",
       "3  Pour la première fois depuis la création des I...  2022-06-02   \n",
       "4  Les levées des capitaux se sont établies à plu...  2022-06-01   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.challenge.ma/maroc-emirats-arabes-...  \n",
       "1  https://www.challenge.ma/banques-les-prets-acc...  \n",
       "2  https://www.challenge.ma/la-cgem-met-le-cap-su...  \n",
       "3  https://www.challenge.ma/le-maroc-pays-africai...  \n",
       "4  https://www.challenge.ma/marche-des-capitaux-l...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(r\"..\\Scraping\\Raw_data\\1.csv\", sep=\",\")\n",
    "df2 = pd.read_csv(r\"..\\Scraping\\Raw_data\\2.csv\", sep=\",\")\n",
    "df3 = pd.read_csv(r\"..\\Scraping\\Raw_data\\3.csv\", sep=\",\")\n",
    "dataset = pd.concat([df1, df2, df3])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5314702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Confédération générale des entreprises du M...</td>\n",
       "      <td>2022-06-09</td>\n",
       "      <td>https://www.challenge.ma/maroc-emirats-arabes-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selon Bank Al-Maghrib, l’encours du crédit ban...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/banques-les-prets-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dans le cadre des Rencontres du Livre Blanc, l...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/la-cgem-met-le-cap-su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pour la première fois depuis la création des I...</td>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>https://www.challenge.ma/le-maroc-pays-africai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les levées des capitaux se sont établies à plu...</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>https://www.challenge.ma/marche-des-capitaux-l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        date  \\\n",
       "0  La Confédération générale des entreprises du M...  2022-06-09   \n",
       "1  Selon Bank Al-Maghrib, l’encours du crédit ban...  2022-06-03   \n",
       "2  Dans le cadre des Rencontres du Livre Blanc, l...  2022-06-03   \n",
       "3  Pour la première fois depuis la création des I...  2022-06-02   \n",
       "4  Les levées des capitaux se sont établies à plu...  2022-06-01   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.challenge.ma/maroc-emirats-arabes-...  \n",
       "1  https://www.challenge.ma/banques-les-prets-acc...  \n",
       "2  https://www.challenge.ma/la-cgem-met-le-cap-su...  \n",
       "3  https://www.challenge.ma/le-maroc-pays-africai...  \n",
       "4  https://www.challenge.ma/marche-des-capitaux-l...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c4b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du corpus = 74 Documents\n"
     ]
    }
   ],
   "source": [
    "corpus = dataset.text.values.tolist()\n",
    "print(\"Taille du corpus = \"+str(len(corpus))+\" Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24aca84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocessing(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff190cd",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "178a2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build TFIDF features on train reviews with a specifique vocabulary\n",
    "corpus_lemmatized=tokenize_text(corpus) \n",
    "id2word = corpora.Dictionary(corpus_lemmatized)\n",
    "vocabulaire=id2word.token2id #get vocabulary dict where keys are terms and values are indices in the feature matrix\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0,sublinear_tf=True,lowercase=True,ngram_range=(1,2),vocabulary=vocabulaire)\n",
    "tfidf_train_features = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90ae8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1133b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "tfidf_train_features_array = tfidf_train_features.toarray()\n",
    "tf_idf_frame = pd.DataFrame(tfidf_train_features_array,columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efa68419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-</th>\n",
       "      <th>abdelmajid</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>actuel</th>\n",
       "      <th>affaire</th>\n",
       "      <th>affirmer</th>\n",
       "      <th>ajoutee</th>\n",
       "      <th>ajouter</th>\n",
       "      <th>albare</th>\n",
       "      <th>algerienn</th>\n",
       "      <th>...</th>\n",
       "      <th>migrate</th>\n",
       "      <th>outsourcia</th>\n",
       "      <th>reagissant</th>\n",
       "      <th>relais</th>\n",
       "      <th>sp</th>\n",
       "      <th>spe</th>\n",
       "      <th>sujette</th>\n",
       "      <th>suspensif</th>\n",
       "      <th>trajectoire</th>\n",
       "      <th>usuel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068419</td>\n",
       "      <td>0.115843</td>\n",
       "      <td>0.039628</td>\n",
       "      <td>0.052164</td>\n",
       "      <td>0.054862</td>\n",
       "      <td>0.046165</td>\n",
       "      <td>0.041909</td>\n",
       "      <td>0.068419</td>\n",
       "      <td>0.068419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     -  abdelmajid  abdullah    actuel   affaire  affirmer   ajoutee  \\\n",
       "0  0.0    0.068419  0.115843  0.039628  0.052164  0.054862  0.046165   \n",
       "1  0.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    ajouter    albare  algerienn  ...  migrate  outsourcia  reagissant  \\\n",
       "0  0.041909  0.068419   0.068419  ...      0.0         0.0         0.0   \n",
       "1  0.000000  0.000000   0.000000  ...      0.0         0.0         0.0   \n",
       "2  0.000000  0.000000   0.000000  ...      0.0         0.0         0.0   \n",
       "3  0.000000  0.000000   0.000000  ...      0.0         0.0         0.0   \n",
       "4  0.000000  0.000000   0.000000  ...      0.0         0.0         0.0   \n",
       "\n",
       "   relais   sp  spe  sujette  suspensif  trajectoire  usuel  \n",
       "0     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "1     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "2     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "3     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "4     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "\n",
       "[5 rows x 4753 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c50bb38",
   "metadata": {},
   "source": [
    "## bigrams processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3cfdf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bigrams = make_bigrams(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc880138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[]\n",
    "for bigrams in list_bigrams :\n",
    "    l.append(len(bigrams))\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df544bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_frame_bigrams(list_bigrams):\n",
    "    \"\"\"Create DataFrame with list of bigrams to be used in association rule learning with R\"\"\"\n",
    "    \n",
    "    # Create a list of column names\n",
    "    columns_bigrams_df = []\n",
    "    for bigrams in list_bigrams :\n",
    "        for bigram in bigrams :\n",
    "            if bigram not in columns_bigrams_df :\n",
    "                columns_bigrams_df.append(bigram)\n",
    "    \n",
    "    # Create dataframe with bigrams\n",
    "    data_frame_bigrams = []\n",
    "    for bigrams in list_bigrams :\n",
    "        l_bigrams=[0 for i in range(len(columns_bigrams_df))]\n",
    "        for bigram in bigrams :\n",
    "            l_bigrams[columns_bigrams_df.index(bigram)]=1\n",
    "        data_frame_bigrams.append(l_bigrams)\n",
    "    \n",
    "    df = pd.DataFrame(data_frame_bigrams,columns=columns_bigrams_df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def  data_frame_tf_idf_bigrams(list_bigrams):\n",
    "    \"\"\"Create TF-IDF DataFrame with list of bigrams that contains the columns TF and IDF of each bigram\"\"\"\n",
    "    \n",
    "    # Create a list of column names\n",
    "    \n",
    "    unique_bigrams_columns = []\n",
    "    intermediate = []\n",
    "    list_tf_idf_bigrams = []\n",
    "    for bigrams in list_bigrams :\n",
    "        for bigram in bigrams :\n",
    "            if bigram not in unique_bigrams_columns :\n",
    "                unique_bigrams_columns.extend(bigram)\n",
    "                intermediate.extend([bigram,bigram])\n",
    "                list_tf_idf_bigrams.extend([\"TF\",\"IDF\"])\n",
    "\n",
    "    # list of column names ==> 2 indexes : bigram and TF/IDF\n",
    "    columns_bigrams_df = [intermediate,list_tf_idf_bigrams] \n",
    "\n",
    "    index=[\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\n",
    "    \"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\",\"C27\",\"C28\",\"C29\",\"C30\",\"C31\",\"C32\",\n",
    "    \"C33\",\"C34\",\"C35\",\"C36\",\"C37\",\"C38\",\"C39\",\"C40\",\"C41\",\"C42\",\"C43\",\"C44\",\"C45\",\"C46\",\"C47\",\"C48\",\"C49\",\"C50\",\n",
    "    \"C51\",\"C52\",\"C53\",\"C54\",\"C55\",\"C56\",\"C57\",\"C58\",\"C59\",\"C60\",\"C61\",\"C62\",\"C63\",\"C64\",\"C65\",\"C66\",\"C67\",\"C68\",\"C69\",\n",
    "    \"C70\",\"C71\",\"C72\",\"C73\",\"C74\"]\n",
    "    \n",
    "    # Create tf-idf dataframe with bigrams\n",
    "    data_frame_bigrams = []\n",
    "\n",
    "    for bigrams in list_bigrams :\n",
    "        l_bigrams=[]\n",
    "        for bigram in unique_bigrams_columns :\n",
    "            l_bigrams.extend([tf_function(bigrams,bigram),idf_function(list_bigrams,bigram)])\n",
    "        data_frame_bigrams.append(l_bigrams)\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame(data_frame_bigrams,columns=columns_bigrams_df,index=index)\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2229561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigrams = data_frame_bigrams(list_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f40c6c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_bigrams = data_frame_tf_idf_bigrams(list_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0713e1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">a</th>\n",
       "      <th colspan=\"2\" halign=\"left\">b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>TF</th>\n",
       "      <th>IDF</th>\n",
       "      <th>TF</th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data1</th>\n",
       "      <td>0.180393</td>\n",
       "      <td>0.343482</td>\n",
       "      <td>0.441857</td>\n",
       "      <td>0.616847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data2</th>\n",
       "      <td>0.381456</td>\n",
       "      <td>0.794810</td>\n",
       "      <td>0.965859</td>\n",
       "      <td>0.682141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              a                   b          \n",
       "             TF       IDF        TF       IDF\n",
       "data1  0.180393  0.343482  0.441857  0.616847\n",
       "data2  0.381456  0.794810  0.965859  0.682141"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = ['a', 'a', 'b', 'b']\n",
    "l2 = [\"TF\", \"IDF\", \"TF\", \"IDF\"]\n",
    "l=[l1,l2]\n",
    "pd.DataFrame(np.random.rand(2, 4),index=[\"data1\" , \"data2\"],columns=l)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe697844cfe8a1028ac8cd95af0dad884ddcd4494e6159c72486af12df7aa875"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "19084cb2a221719542bd94e588e69917ee95725b301fa5f2df470f319dde6836"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
