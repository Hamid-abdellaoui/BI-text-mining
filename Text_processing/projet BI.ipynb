{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee645fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des bibiotheque necessaire\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "import re\n",
    "#from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import fr_core_news_md #import spacy french stemmer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d16ee797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list(STOP_WORDS)).index(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12e3fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "def tokenize_text(corpus):\n",
    "    tokensCorpus=[]\n",
    "    for doc in corpus:\n",
    "        doc_tokens = word_tokenize(doc)\n",
    "        tokensCorpus.append(doc_tokens)\n",
    "    return tokensCorpus\n",
    "\n",
    "# removing stopwords\n",
    "def remove_stopwords(corpus):\n",
    "    filtered_corpus=[]\n",
    "    for tokens in corpus:\n",
    "        #french_sw = stopwords.words('french') \n",
    "        french_sw=list(STOP_WORDS) #get french stopwords\n",
    "        filtered_tokens = [token for token in tokens.split() if token not in french_sw and len(token)>2]\n",
    "        filtred_text=' '.join(filtered_tokens) #reforme le text du documents separé par espace\n",
    "        filtered_corpus.append(filtred_text)\n",
    "    return filtered_corpus\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    list_bigrams=[]\n",
    "    for doc in texts :\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(doc)\n",
    "        # bigrams \n",
    "        bigrams = [w for w in ngrams(tokens,n=2)]\n",
    "        list_bigrams.append(bigrams)\n",
    "    return list_bigrams\n",
    "\n",
    "def unique_bigrams(texts):\n",
    "    list_bigrams = make_bigrams(texts)\n",
    "    list_unique_bigrams=[]\n",
    "    for list_bigram in list_bigrams :\n",
    "        list_unique_bigrams.append(list(set(list_bigram)))\n",
    "    return list_unique_bigrams\n",
    "\n",
    "def count_unique_bigrams(texts):\n",
    "    list_unique_bigrams = unique_bigrams(texts)\n",
    "    list_count_unique_bigrams = []\n",
    "    for list_unique_bigram in list_unique_bigrams :\n",
    "        list_count_unique_bigrams.append(len(list_unique_bigram))\n",
    "    return list_count_unique_bigrams\n",
    "\n",
    "def tf_function(bigrams,bigram):\n",
    "    n=len(bigrams) #nombre de bigrams uniques dans le texte\n",
    "    m= bigrams.count(bigram) # nombre d'occurence du bigram en parametre dans la liste de bigrams\n",
    "    return  m/(n+1)\n",
    "\n",
    "def idf_function(whole_bigram,bigram):\n",
    "    list_idf = []\n",
    "    s = 0\n",
    "    for bigrams in whole_bigram:  \n",
    "        if bigram in bigrams:\n",
    "            s += 1\n",
    "    return np.log((len(whole_bigram)/(s+1))+1)\n",
    "\n",
    "def tf_idf_function(whole_bigram,bigrams,bigram):\n",
    "    return tf_function(bigrams,bigram) * idf_function(whole_bigram,bigram)\n",
    "\n",
    "#output French accents correctly\n",
    "def convert_accents(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore')\n",
    "\n",
    "#convertisse les documents en minuscule\n",
    "def lower_text(corpus):\n",
    "    LowerCorpus=[]\n",
    "    for doc in corpus:\n",
    "        lowerDoc=str(doc).lower() #convertissent le texte en minuscules\n",
    "        lowerDoc=convert_accents(lowerDoc).decode(\"utf-8\") #supprimes les accents\n",
    "        LowerCorpus.append(lowerDoc)\n",
    "    return LowerCorpus\n",
    "\n",
    "#supprimes caracteres speciaux\n",
    "def remove_characters(corpus,keep_apostrophes=True):\n",
    "    filtered_corpus=[]\n",
    "    for doc in corpus:\n",
    "        doc = doc.strip()\n",
    "        if keep_apostrophes:\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            doc =re.sub(\"l\\'\",\"\",doc)\n",
    "            doc =re.sub(\"d\\'\",\"\",doc)\n",
    "            PATTERN = r'[?|$|&|*|-|!|%|@|(|)|~|\\d]'\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "            filtered_corpus.append(filtered_doc)\n",
    "        else:\n",
    "            PATTERN = r'[^a-zA-Z ]'\n",
    "            #supprimes les urls\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "        \n",
    "            filtered_corpus.append(filtered_doc)\n",
    "    return filtered_corpus\n",
    "\n",
    "#recuperer les mots qui apparaissent dans plusieurs documents\n",
    "def get_mostCommonWords(corpus,max_freq=100):\n",
    "    vocabulaire=dict() #dictionnaire qui va contenir le nombre d'occurence des mots dans les documents\n",
    "    for doc in corpus:\n",
    "        for word in set(doc.split()): #recupere les mots unique de chaque documents\n",
    "            if word in vocabulaire:\n",
    "                vocabulaire[word]+=1\n",
    "            else:\n",
    "                vocabulaire[word]=1\n",
    "    \n",
    "    #recupere les dont le nombre d'occurences dans les documents > max_freq\n",
    "    mostCommonsWord=[word for word,value in vocabulaire.items() if value>max_freq ]\n",
    "        \n",
    "    return mostCommonsWord\n",
    "\n",
    "#lemmatisation\n",
    "def lemm_tokens(corpus):\n",
    "    \n",
    "    nlp = fr_core_news_md.load() #initialisation du model \"fr_core_news_md\" de spacy\n",
    "    allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    corpus_lemms=[]\n",
    "    \n",
    "    for document in corpus:\n",
    "        doc = nlp(document)\n",
    "        lemms=[token.lemma_ for token in doc if token.pos_ in allowed_postags] #recupere les lemms des tokens\n",
    "        text=' '.join(lemms) #reforme le text du documents separé par espace\n",
    "        corpus_lemms.append(text)\n",
    "            \n",
    "    return corpus_lemms\n",
    "#fonction qui supprimes les documents vides ou tres courte\n",
    "def remove_shortDocument(corpus,min_length=3):\n",
    "    filtred_corpus=[]\n",
    "    idx_doc=[]\n",
    "    for idx,doc in enumerate(corpus):\n",
    "        \n",
    "        if len(doc.split())>min_length:\n",
    "            filtred_corpus.append(doc)\n",
    "            idx_doc.append(idx)\n",
    "        \n",
    "    \n",
    "    return filtred_corpus,idx_doc\n",
    "\n",
    "# extracts topics with their terms and weights\n",
    "# format is Topic N: [(term1, weight1), ..., (termn, weightn)]        \n",
    "def get_topics_terms_weights(weights, feature_names):\n",
    "    feature_names = np.array(feature_names)\n",
    "    \n",
    "    #trie les indices des mots de chaque topics selon la poids du mots dans le topics\n",
    "    sorted_indices = np.array([list(row[::-1]) for row in np.argsort(np.abs(weights))])\n",
    "    \n",
    "    #trie les poids des mots de chaques topics,en recuperant les poids des indices deja triée\n",
    "    sorted_weights = np.array([list(wt[index]) for wt, index in zip(weights,sorted_indices)])\n",
    "    \n",
    "    #recupres les mots selon leurs indices deja triée\n",
    "    sorted_terms = np.array([list(feature_names[row]) for row in sorted_indices])\n",
    "    \n",
    "    #concatene chaque mots et sa poids sous formes de tuple (mot,poids)\n",
    "    topics = [np.vstack((terms.T, term_weights.T)).T for terms, term_weights in zip(sorted_terms, sorted_weights)]     \n",
    "    \n",
    "    return topics\n",
    "\n",
    "def compute_coherence_values(tfidf_train_features,feature_names,corpus,data_lemmatized,id2word,max_term=20,limit=50, start=5, step=5):\n",
    "    \"\"\"\n",
    "    Calcul la coherence UMass pour different nombre de topic\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    tfidf_train_features : features tf-idf qu'on va utiliser pour entrainer chaque model\n",
    "    feature_names : ensemble des mots contenue dans la matrice tf-idf\n",
    "    corpus: corpus de base qui contients les documents sous forme de texte\n",
    "    max_term: nombre maximal de mots qu'on va prendre pour calculé la coherence de chaque topic\n",
    "    data_lemmatized: corpus sous forme de tokens\n",
    "    id2word:vocabulaire du corpus au format de gensim\n",
    "    max_term:le nombre de termes qu'on va prendre dans chaque topic pour calculer la Coherence\n",
    "    limit : Nombre maximal de topics qu'on va tester\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    best_model : le model qui contient le plus grande coherence\n",
    "    coherence_values : Valeurs des Cohérences correspondant au modèle avec le nombre respectif de sujets\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model_list = [] #listes qui va contenir les modeles tester\n",
    "    coherence_values = [] #liste qui contenir les coherences de chaque models\n",
    "    # Term Document Frequency\n",
    "    \n",
    "    common_corpus = [id2word.doc2bow(text) for text in data_lemmatized] #recupere la matrice bog of word du corpus sous le format de gensim\n",
    "\n",
    "    \n",
    "    #print(coherence)\n",
    "    for num_topics in range(start, limit, step):\n",
    "        \n",
    "        model=NMF(n_components=num_topics,random_state=42) #model MNF\n",
    "        model.fit(tfidf_train_features)\n",
    "        weights = model.components_ #recupere les poids\n",
    "        \n",
    "        model_list.append(model) #ajoute le model la liste des models utilisé\n",
    "        \n",
    "        \n",
    "        topics=get_topics_terms_weights(weights,feature_names)\n",
    "        \n",
    "        topic_terms=getTopicTerms(topics)#recupere les mot des de chaque topics\n",
    "        \n",
    "        topic_terms=[topics[:max_term] for topics in topic_terms] #recupere les  \"max_term\" termes avec les plus grandes poids\n",
    "        \n",
    "        #calcule du Coherence UMass\n",
    "        cm = CoherenceModel(topics=topic_terms,corpus=common_corpus, dictionary=id2word, coherence='u_mass')\n",
    "        coherence = cm.get_coherence()\n",
    "        coherence_values.append(coherence)\n",
    "    \n",
    "    idx_max=np.array(coherence_values).argmax() #recupere l'indice du model qui possede le plus grands coherence\n",
    "    best_model=model_list[idx_max] #recupere le meilleur models\n",
    "    \n",
    "\n",
    "    return best_model,coherence_values\n",
    "\n",
    "def getTopicTerms(pos_topics):\n",
    "    \"\"\"\n",
    "    Fonction qui retourne l'ensemble des mots qui compose chaque topics\n",
    "    ----Input----\n",
    "    pos_topics: ensemble des topics qui contients les mots et leurs poids\n",
    "    ---output---\n",
    "    topic_terms : ensemble des mots des topics\n",
    "    \n",
    "    \"\"\"\n",
    "    topic_terms=[]\n",
    "    for topic in pos_topics:\n",
    "        #topic=topic[:max_term] #recupere les \"max_term\" premiere mots et leurs poids\n",
    "        terms=[]\n",
    "        for doc in topic:\n",
    "            terms.append(doc[0]) #recupere justes les mots sans les poids\n",
    "        \n",
    "        topic_terms.append(terms) #ajoute l'ensemble des mots\n",
    "    \n",
    "    return topic_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c806910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    \n",
    "    corpus=remove_characters(corpus)\n",
    "    corpus=lower_text(corpus)\n",
    "    corpus=remove_stopwords(corpus)\n",
    "    corpus=lemm_tokens(corpus)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "145a7873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pêche. Hausse des débarquements de 14% à fin m...</td>\n",
       "      <td>Les débarquements des produits commercialisés ...</td>\n",
       "      <td>2022-06-14</td>\n",
       "      <td>https://www.challenge.ma/peche-hausse-des-deba...</td>\n",
       "      <td>https://www.challenge.ma/wp-content/uploads/20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trésorerie. Le déficit budgétaire à 14,6 MMDH ...</td>\n",
       "      <td>La situation des charges et ressources du Trés...</td>\n",
       "      <td>2022-06-13</td>\n",
       "      <td>https://www.challenge.ma/tresorerie-le-deficit...</td>\n",
       "      <td>https://www.challenge.ma/wp-content/uploads/20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ouverture à Rabat de la réunion de haut niveau...</td>\n",
       "      <td>Les travaux de la réunion de haut niveau du co...</td>\n",
       "      <td>2022-06-13</td>\n",
       "      <td>https://www.challenge.ma/ouverture-a-rabat-de-...</td>\n",
       "      <td>https://www.challenge.ma/wp-content/uploads/20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maroc/Emirats Arabes Unis. Signature d’un MoU ...</td>\n",
       "      <td>La Confédération générale des entreprises du M...</td>\n",
       "      <td>2022-06-09</td>\n",
       "      <td>https://www.challenge.ma/maroc-emirats-arabes-...</td>\n",
       "      <td>https://www.challenge.ma/wp-content/uploads/20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prêts bancaires. Plus de 900 milliards de DH s...</td>\n",
       "      <td>Selon Bank Al-Maghrib, l’encours du crédit ban...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/banques-les-prets-acc...</td>\n",
       "      <td>https://www.challenge.ma/wp-content/uploads/20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               titre  \\\n",
       "0  Pêche. Hausse des débarquements de 14% à fin m...   \n",
       "1  Trésorerie. Le déficit budgétaire à 14,6 MMDH ...   \n",
       "2  Ouverture à Rabat de la réunion de haut niveau...   \n",
       "3  Maroc/Emirats Arabes Unis. Signature d’un MoU ...   \n",
       "4  Prêts bancaires. Plus de 900 milliards de DH s...   \n",
       "\n",
       "                                                text       date  \\\n",
       "0  Les débarquements des produits commercialisés ... 2022-06-14   \n",
       "1  La situation des charges et ressources du Trés... 2022-06-13   \n",
       "2  Les travaux de la réunion de haut niveau du co... 2022-06-13   \n",
       "3  La Confédération générale des entreprises du M... 2022-06-09   \n",
       "4  Selon Bank Al-Maghrib, l’encours du crédit ban... 2022-06-03   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.challenge.ma/peche-hausse-des-deba...   \n",
       "1  https://www.challenge.ma/tresorerie-le-deficit...   \n",
       "2  https://www.challenge.ma/ouverture-a-rabat-de-...   \n",
       "3  https://www.challenge.ma/maroc-emirats-arabes-...   \n",
       "4  https://www.challenge.ma/banques-les-prets-acc...   \n",
       "\n",
       "                                                 img  \n",
       "0  https://www.challenge.ma/wp-content/uploads/20...  \n",
       "1  https://www.challenge.ma/wp-content/uploads/20...  \n",
       "2  https://www.challenge.ma/wp-content/uploads/20...  \n",
       "3  https://www.challenge.ma/wp-content/uploads/20...  \n",
       "4  https://www.challenge.ma/wp-content/uploads/20...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(r\"..\\Automating\\airflow\\data\\Dataset\\data.csv\", sep=\",\", parse_dates=['date'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5314702",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['period']=dataset['date'].dt.to_period('M')\n",
    "dataset.sort_values(by=\"date\" ,inplace=True)\n",
    "mois_mai = list(set(dataset[\"period\"]))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18666480",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset[\"period\"]== mois_mai]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26c4b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du corpus = 231 Documents\n"
     ]
    }
   ],
   "source": [
    "corpus = dataset.text.values.tolist()\n",
    "print(\"Taille du corpus = \"+str(len(corpus))+\" Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2204c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocessing(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2880ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def corpuses_NMF (dataset):\\n    corpus = dataset.text.values.tolist()\\n    corpuses_NMF = []\\n    l_periods=list(set(dataset.period.values))\\n    l_periods.sort()\\n    for i in range(len(l_periods)-1) :\\n        indice1=dataset.period.values.tolist().index(l_periods[i])\\n        indice2=dataset.period.values.tolist().index(l_periods[i+1])\\n        liste = []\\n        for j in range(indice1,indice2,1):\\n            liste.append(corpus[j]) \\n        corpuses_NMF.append(liste)\\n\\n    dernier_indice=dataset.period.values.tolist().index(l_periods[1])\\n    derniere_corpus = [] \\n    for j in range(dernier_indice,len(dataset),1):\\n        derniere_corpus.append(corpus[j]) \\n    corpuses_NMF.append(derniere_corpus)\\n    return corpuses_NMF'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def corpuses_NMF (dataset):\n",
    "    corpus = dataset.text.values.tolist()\n",
    "    corpuses_NMF = []\n",
    "    l_periods=list(set(dataset.period.values))\n",
    "    l_periods.sort()\n",
    "    for i in range(len(l_periods)-1) :\n",
    "        indice1=dataset.period.values.tolist().index(l_periods[i])\n",
    "        indice2=dataset.period.values.tolist().index(l_periods[i+1])\n",
    "        liste = []\n",
    "        for j in range(indice1,indice2,1):\n",
    "            liste.append(corpus[j]) \n",
    "        corpuses_NMF.append(liste)\n",
    "\n",
    "    dernier_indice=dataset.period.values.tolist().index(l_periods[1])\n",
    "    derniere_corpus = [] \n",
    "    for j in range(dernier_indice,len(dataset),1):\n",
    "        derniere_corpus.append(corpus[j]) \n",
    "    corpuses_NMF.append(derniere_corpus)\n",
    "    return corpuses_NMF'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b4a0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpuses_NMF = dataset.text.values.tolist()\n",
    "corpuses_NMF = preprocessing(corpuses_NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c50bb38",
   "metadata": {},
   "source": [
    "## bigrams processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3cfdf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bigrams = make_bigrams(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3590d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_bigrams(list_bigrams):\n",
    "    \"\"\"Create DataFrame with list of bigrams to be used in association rule learning with R\"\"\"\n",
    "    \n",
    "    # Create a list of column names\n",
    "    columns_bigrams_df = []\n",
    "    for bigrams in list_bigrams :\n",
    "        for bigram in bigrams :\n",
    "            if bigram not in columns_bigrams_df :\n",
    "                columns_bigrams_df.append(bigram)\n",
    "    return columns_bigrams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "68ecf1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_bigrams_df = columns_bigrams(list_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df544bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_frame_bigrams(list_bigrams):\n",
    "    \"\"\"Create DataFrame with list of bigrams to be used in association rule learning with R\"\"\"\n",
    "    \n",
    "    # Create a list of column names\n",
    "    columns_bigrams_df = []\n",
    "    for bigrams in list_bigrams :\n",
    "        for bigram in bigrams :\n",
    "            if bigram not in columns_bigrams_df :\n",
    "                columns_bigrams_df.append(bigram)\n",
    "    \n",
    "    # Create dataframe with bigrams\n",
    "    data_frame_bigrams = []\n",
    "    for bigrams in list_bigrams :\n",
    "        l_bigrams=[0 for i in range(len(columns_bigrams_df))]\n",
    "        for bigram in bigrams :\n",
    "            l_bigrams[columns_bigrams_df.index(bigram)]=1\n",
    "        data_frame_bigrams.append(l_bigrams)\n",
    "    \n",
    "    df = pd.DataFrame(data_frame_bigrams,columns=columns_bigrams_df)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2229561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigrams = data_frame_bigrams(list_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2773d434",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_bigrams.to_csv('../Automating/airflow/data/Outputs/appriori.csv',\n",
    " index=False, header= columns_bigrams_df , sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0452aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ define process function ########################## \n",
    "\n",
    "def process_corpus(corpus):\n",
    "    \"\"\"retourne une liste output\"\"\"\n",
    "    ########### former la matrice tf-idf #############################\n",
    "    corpus_lemmatized=tokenize_text(corpus) \n",
    "    id2word = corpora.Dictionary(corpus_lemmatized)\n",
    "    vocabulaire=id2word.token2id \n",
    "    tfidf = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0,sublinear_tf=True,\n",
    "                        lowercase=True,ngram_range=(1,2),vocabulary=vocabulaire)\n",
    "    tfidf_train_features = tfidf.fit_transform(corpus)\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    ################# Topic Modelling : NMF ############################################\n",
    "    total_topics = 10\n",
    "    pos_nmf=NMF(n_components=total_topics,random_state=42,l1_ratio=0.2,max_iter=200)\n",
    "    pos_nmf.fit(tfidf_train_features) \n",
    "    pos_weights = pos_nmf.components_\n",
    "    pos_topics = get_topics_terms_weights(pos_weights, feature_names)\n",
    "    topic_terms=getTopicTerms(pos_topics)\n",
    "    topic_terms=[topics[:20] for topics in topic_terms] \n",
    "    common_corpus = [id2word.doc2bow(text) for text in corpus_lemmatized] \n",
    "    ####### choix du meilleur model ###################################\n",
    "    cm = CoherenceModel(topics=topic_terms,corpus=common_corpus, dictionary=id2word, \n",
    "               coherence='u_mass')\n",
    "    coherence = cm.get_coherence()\n",
    "    best_model,coherence_values=compute_coherence_values(tfidf_train_features,\n",
    "    feature_names,corpus,corpus_lemmatized,id2word,max_term=20,limit=50)\n",
    "    ######## résultat du modèle optimal ##############################\n",
    "    total_topics=best_model.n_components \n",
    "    weights = best_model.components_ \n",
    "    topics = get_topics_terms_weights(weights,feature_names)\n",
    "    ##### Data_For_Plot ########################################################\n",
    "    dates=pd.to_datetime(dataset.loc[:,\"date\"].values)\n",
    "    doc_topic_dist = best_model.transform(tfidf_train_features) \n",
    "    labels=getTopicTerms(topics)\n",
    "    labels=[\",\".join(topic_term[:5]) for topic_term in labels] \n",
    "    df=pd.DataFrame({\"text\":corpus,\"Date\":dates,\"doc_num\":np.arange(len(corpus))})\n",
    "    stories=df.groupby(\"doc_num\")[\"text\",\"Date\"].min().reset_index() \n",
    "    story_topics_for_plot=pd.DataFrame(dict(doc_num=np.arange(doc_topic_dist.shape[0])))\n",
    "    for idx in range(len(labels)):\n",
    "        story_topics_for_plot[labels[idx]] = doc_topic_dist[:, idx]\n",
    "    trends = stories.merge(story_topics_for_plot, on='doc_num')\n",
    "    mass = lambda x: ((x) * 1.0).sum() / x.shape[0]  \n",
    "    window = 10\n",
    "    trend_indice = min(len(labels),5)\n",
    "    aggs = {labels[i]: mass for i in range(trend_indice) }\n",
    "    data_for_plot=trends.groupby(trends['Date'].dt.date).agg(aggs).rolling(window).mean()\n",
    "\n",
    "    ######### output de chaque corpus ###############################\n",
    "    output=[story_topics_for_plot,data_for_plot]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48e0faa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "C:\\Users\\ORIGIN~1\\AppData\\Local\\Temp/ipykernel_92/1434948719.py:38: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  stories=df.groupby(\"doc_num\")[\"text\",\"Date\"].min().reset_index()\n"
     ]
    }
   ],
   "source": [
    "df_for_plots = process_corpus(corpuses_NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e470a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_0 = df_for_plots[0].columns\n",
    "columns_1 = df_for_plots[1].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "526e6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_plots[0].to_csv('../Automating/airflow/data/Outputs/topics.csv',\n",
    "index=False , header= columns_0 , sep=',')\n",
    "df_for_plots[1].to_csv('../Automating/airflow/data/Outputs/trends.csv',\n",
    "index=False, header = columns_1 , sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe697844cfe8a1028ac8cd95af0dad884ddcd4494e6159c72486af12df7aa875"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
