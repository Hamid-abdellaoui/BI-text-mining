{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee645fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des bibiotheque necessaire\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "import re\n",
    "#from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import fr_core_news_md #import spacy french stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e3fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "def tokenize_text(corpus):\n",
    "    tokensCorpus=[]\n",
    "    for doc in corpus:\n",
    "        doc_tokens = word_tokenize(doc)\n",
    "        tokensCorpus.append(doc_tokens)\n",
    "    return tokensCorpus\n",
    "\n",
    "# removing stopwords\n",
    "def remove_stopwords(corpus):\n",
    "    filtered_corpus=[]\n",
    "    for tokens in corpus:\n",
    "        #french_sw = stopwords.words('french') \n",
    "        french_sw=list(STOP_WORDS) #get french stopwords\n",
    "        filtered_tokens = [token for token in tokens.split() if token not in french_sw and len(token)>2]\n",
    "        filtred_text=' '.join(filtered_tokens) #reforme le text du documents separé par espace\n",
    "        filtered_corpus.append(filtred_text)\n",
    "    return filtered_corpus\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    list_bigrams=[]\n",
    "    for doc in texts :\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(doc)\n",
    "        # bigrams \n",
    "        bigrams = [w for w in ngrams(tokens,n=2)]\n",
    "        list_bigrams.append(bigrams)\n",
    "    return list_bigrams\n",
    "\n",
    "def unique_bigrams(texts):\n",
    "    list_bigrams = make_bigrams(texts)\n",
    "    list_unique_bigrams=[]\n",
    "    for list_bigram in list_bigrams :\n",
    "        list_unique_bigrams.append(list(set(list_bigram)))\n",
    "    return list_unique_bigrams\n",
    "\n",
    "def count_unique_bigrams(texts):\n",
    "    list_unique_bigrams = unique_bigrams(texts)\n",
    "    list_count_unique_bigrams = []\n",
    "    for list_unique_bigram in list_unique_bigrams :\n",
    "        list_count_unique_bigrams.append(len(list_unique_bigram))\n",
    "    return list_count_unique_bigrams\n",
    "\n",
    "def tf_function(text,word):\n",
    "    n=len(word_tokenize(text)) #nombre de mots dans le texte\n",
    "    m= word_tokenize(text).count(word) # nombre d'occurence du mot en parametre dans le texte\n",
    "    return  m/n\n",
    "\n",
    "def idf_function(texts,word):\n",
    "    list_idf = []\n",
    "    s = 0\n",
    "    for text in  texts :\n",
    "        if word in word_tokenize(text):\n",
    "            s = s + 1\n",
    "    return np.log((len(texts)+1)/(s+1))+1\n",
    "\n",
    "def tf_idf_function(texts,text,word):\n",
    "    return tf_function(text,word) * idf_function(texts,word)\n",
    "\n",
    "#output French accents correctly\n",
    "def convert_accents(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore')\n",
    "\n",
    "#convertisse les documents en minuscule\n",
    "def lower_text(corpus):\n",
    "    LowerCorpus=[]\n",
    "    for doc in corpus:\n",
    "        lowerDoc=str(doc).lower() #convertissent le texte en minuscules\n",
    "        lowerDoc=convert_accents(lowerDoc).decode(\"utf-8\") #supprimes les accents\n",
    "        LowerCorpus.append(lowerDoc)\n",
    "    return LowerCorpus\n",
    "\n",
    "#supprimes caracteres speciaux\n",
    "def remove_characters(corpus,keep_apostrophes=True):\n",
    "    filtered_corpus=[]\n",
    "    for doc in corpus:\n",
    "        doc = doc.strip()\n",
    "        if keep_apostrophes:\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            doc =re.sub(\"l\\'\",\"\",doc)\n",
    "            doc =re.sub(\"d\\'\",\"\",doc)\n",
    "            PATTERN = r'[?|$|&|*|-|!|%|@|(|)|~|\\d]'\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "            filtered_corpus.append(filtered_doc)\n",
    "        else:\n",
    "            PATTERN = r'[^a-zA-Z ]'\n",
    "            #supprimes les urls\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "        \n",
    "            filtered_corpus.append(filtered_doc)\n",
    "    return filtered_corpus\n",
    "\n",
    "#recuperer les mots qui apparaissent dans plusieurs documents\n",
    "def get_mostCommonWords(corpus,max_freq=100):\n",
    "    vocabulaire=dict() #dictionnaire qui va contenir le nombre d'occurence des mots dans les documents\n",
    "    for doc in corpus:\n",
    "        for word in set(doc.split()): #recupere les mots unique de chaque documents\n",
    "            if word in vocabulaire:\n",
    "                vocabulaire[word]+=1\n",
    "            else:\n",
    "                vocabulaire[word]=1\n",
    "    \n",
    "    #recupere les dont le nombre d'occurences dans les documents > max_freq\n",
    "    mostCommonsWord=[word for word,value in vocabulaire.items() if value>max_freq ]\n",
    "        \n",
    "    return mostCommonsWord\n",
    "\n",
    "#lemmatisation\n",
    "def lemm_tokens(corpus):\n",
    "    \n",
    "    nlp = fr_core_news_md.load() #initialisation du model \"fr_core_news_md\" de spacy\n",
    "    allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    corpus_lemms=[]\n",
    "    \n",
    "    for document in corpus:\n",
    "        doc = nlp(document)\n",
    "        lemms=[token.lemma_ for token in doc if token.pos_ in allowed_postags] #recupere les lemms des tokens\n",
    "        text=' '.join(lemms) #reforme le text du documents separé par espace\n",
    "        corpus_lemms.append(text)\n",
    "            \n",
    "    return corpus_lemms\n",
    "#fonction qui supprimes les documents vides ou tres courte\n",
    "def remove_shortDocument(corpus,min_length=3):\n",
    "    filtred_corpus=[]\n",
    "    idx_doc=[]\n",
    "    for idx,doc in enumerate(corpus):\n",
    "        \n",
    "        if len(doc.split())>min_length:\n",
    "            filtred_corpus.append(doc)\n",
    "            idx_doc.append(idx)\n",
    "        \n",
    "    \n",
    "    return filtred_corpus,idx_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c806910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    \n",
    "    corpus=remove_characters(corpus)\n",
    "    corpus=lower_text(corpus)\n",
    "    corpus=remove_stopwords(corpus)\n",
    "    corpus=lemm_tokens(corpus)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145a7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5314702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Confédération générale des entreprises du M...</td>\n",
       "      <td>2022-06-09</td>\n",
       "      <td>https://www.challenge.ma/maroc-emirats-arabes-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selon Bank Al-Maghrib, l’encours du crédit ban...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/banques-les-prets-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dans le cadre des Rencontres du Livre Blanc, l...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/la-cgem-met-le-cap-su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pour la première fois depuis la création des I...</td>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>https://www.challenge.ma/le-maroc-pays-africai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Fédération Nationale des Promoteurs Immobil...</td>\n",
       "      <td>2022-06-11</td>\n",
       "      <td>https://lematin.ma/express/2022/2eme-edition-s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        date  \\\n",
       "0  La Confédération générale des entreprises du M...  2022-06-09   \n",
       "1  Selon Bank Al-Maghrib, l’encours du crédit ban...  2022-06-03   \n",
       "2  Dans le cadre des Rencontres du Livre Blanc, l...  2022-06-03   \n",
       "3  Pour la première fois depuis la création des I...  2022-06-02   \n",
       "4  La Fédération Nationale des Promoteurs Immobil...  2022-06-11   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.challenge.ma/maroc-emirats-arabes-...  \n",
       "1  https://www.challenge.ma/banques-les-prets-acc...  \n",
       "2  https://www.challenge.ma/la-cgem-met-le-cap-su...  \n",
       "3  https://www.challenge.ma/le-maroc-pays-africai...  \n",
       "4  https://lematin.ma/express/2022/2eme-edition-s...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c4b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du corpus = 24 Documents\n"
     ]
    }
   ],
   "source": [
    "corpus = dataset.text.values.tolist()\n",
    "print(\"Taille du corpus = \"+str(len(corpus))+\" Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24aca84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocessing(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff190cd",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "178a2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build TFIDF features on train reviews with a specifique vocabulary\n",
    "corpus_lemmatized=tokenize_text(corpus) \n",
    "id2word = corpora.Dictionary(corpus_lemmatized)\n",
    "vocabulaire=id2word.token2id #get vocabulary dict where keys are terms and values are indices in the feature matrix\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0,sublinear_tf=True,lowercase=True,ngram_range=(1,2),vocabulary=vocabulaire)\n",
    "tfidf_train_features = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90ae8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1133b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "tfidf_train_features_array = tfidf_train_features.toarray()\n",
    "tf_idf_frame = pd.DataFrame(tfidf_train_features_array,columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efa68419",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_bigrams = make_bigrams(corpus)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfdf5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
