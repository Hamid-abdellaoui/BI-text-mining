{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee645fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des bibiotheque necessaire\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "import re\n",
    "#from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import fr_core_news_md #import spacy french stemmer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e3fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "def tokenize_text(corpus):\n",
    "    tokensCorpus=[]\n",
    "    for doc in corpus:\n",
    "        doc_tokens = word_tokenize(doc)\n",
    "        tokensCorpus.append(doc_tokens)\n",
    "    return tokensCorpus\n",
    "\n",
    "# removing stopwords\n",
    "def remove_stopwords(corpus):\n",
    "    filtered_corpus=[]\n",
    "    for tokens in corpus:\n",
    "        #french_sw = stopwords.words('french') \n",
    "        french_sw=list(STOP_WORDS) #get french stopwords\n",
    "        filtered_tokens = [token for token in tokens.split() if token not in french_sw and len(token)>2]\n",
    "        filtred_text=' '.join(filtered_tokens) #reforme le text du documents separé par espace\n",
    "        filtered_corpus.append(filtred_text)\n",
    "    return filtered_corpus\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    list_bigrams=[]\n",
    "    for doc in texts :\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(doc)\n",
    "        # bigrams \n",
    "        bigrams = [w for w in ngrams(tokens,n=2)]\n",
    "        list_bigrams.append(bigrams)\n",
    "    return list_bigrams\n",
    "\n",
    "def unique_bigrams(texts):\n",
    "    list_bigrams = make_bigrams(texts)\n",
    "    list_unique_bigrams=[]\n",
    "    for list_bigram in list_bigrams :\n",
    "        list_unique_bigrams.append(list(set(list_bigram)))\n",
    "    return list_unique_bigrams\n",
    "\n",
    "def count_unique_bigrams(texts):\n",
    "    list_unique_bigrams = unique_bigrams(texts)\n",
    "    list_count_unique_bigrams = []\n",
    "    for list_unique_bigram in list_unique_bigrams :\n",
    "        list_count_unique_bigrams.append(len(list_unique_bigram))\n",
    "    return list_count_unique_bigrams\n",
    "\n",
    "def tf_function(bigrams,bigram):\n",
    "    n=len(bigrams) #nombre de bigrams uniques dans le texte\n",
    "    m= bigrams.count(bigram) # nombre d'occurence du bigram en parametre dans la liste de bigrams\n",
    "    return  m/(n+1)\n",
    "\n",
    "def idf_function(whole_bigram,bigram):\n",
    "    list_idf = []\n",
    "    s = 0\n",
    "    for bigrams in whole_bigram:  \n",
    "        if bigram in bigrams:\n",
    "            s += 1\n",
    "    return np.log((len(whole_bigram)/(s+1))+1)\n",
    "\n",
    "def tf_idf_function(whole_bigram,bigrams,bigram):\n",
    "    return tf_function(bigrams,bigram) * idf_function(whole_bigram,bigram)\n",
    "\n",
    "#output French accents correctly\n",
    "def convert_accents(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore')\n",
    "\n",
    "#convertisse les documents en minuscule\n",
    "def lower_text(corpus):\n",
    "    LowerCorpus=[]\n",
    "    for doc in corpus:\n",
    "        lowerDoc=str(doc).lower() #convertissent le texte en minuscules\n",
    "        lowerDoc=convert_accents(lowerDoc).decode(\"utf-8\") #supprimes les accents\n",
    "        LowerCorpus.append(lowerDoc)\n",
    "    return LowerCorpus\n",
    "\n",
    "#supprimes caracteres speciaux\n",
    "def remove_characters(corpus,keep_apostrophes=True):\n",
    "    filtered_corpus=[]\n",
    "    for doc in corpus:\n",
    "        doc = doc.strip()\n",
    "        if keep_apostrophes:\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            doc =re.sub(\"l\\'\",\"\",doc)\n",
    "            doc =re.sub(\"d\\'\",\"\",doc)\n",
    "            PATTERN = r'[?|$|&|*|-|!|%|@|(|)|~|\\d]'\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "            filtered_corpus.append(filtered_doc)\n",
    "        else:\n",
    "            PATTERN = r'[^a-zA-Z ]'\n",
    "            #supprimes les urls\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "        \n",
    "            filtered_corpus.append(filtered_doc)\n",
    "    return filtered_corpus\n",
    "\n",
    "#recuperer les mots qui apparaissent dans plusieurs documents\n",
    "def get_mostCommonWords(corpus,max_freq=100):\n",
    "    vocabulaire=dict() #dictionnaire qui va contenir le nombre d'occurence des mots dans les documents\n",
    "    for doc in corpus:\n",
    "        for word in set(doc.split()): #recupere les mots unique de chaque documents\n",
    "            if word in vocabulaire:\n",
    "                vocabulaire[word]+=1\n",
    "            else:\n",
    "                vocabulaire[word]=1\n",
    "    \n",
    "    #recupere les dont le nombre d'occurences dans les documents > max_freq\n",
    "    mostCommonsWord=[word for word,value in vocabulaire.items() if value>max_freq ]\n",
    "        \n",
    "    return mostCommonsWord\n",
    "\n",
    "#lemmatisation\n",
    "def lemm_tokens(corpus):\n",
    "    \n",
    "    nlp = fr_core_news_md.load() #initialisation du model \"fr_core_news_md\" de spacy\n",
    "    allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    corpus_lemms=[]\n",
    "    \n",
    "    for document in corpus:\n",
    "        doc = nlp(document)\n",
    "        lemms=[token.lemma_ for token in doc if token.pos_ in allowed_postags] #recupere les lemms des tokens\n",
    "        text=' '.join(lemms) #reforme le text du documents separé par espace\n",
    "        corpus_lemms.append(text)\n",
    "            \n",
    "    return corpus_lemms\n",
    "#fonction qui supprimes les documents vides ou tres courte\n",
    "def remove_shortDocument(corpus,min_length=3):\n",
    "    filtred_corpus=[]\n",
    "    idx_doc=[]\n",
    "    for idx,doc in enumerate(corpus):\n",
    "        \n",
    "        if len(doc.split())>min_length:\n",
    "            filtred_corpus.append(doc)\n",
    "            idx_doc.append(idx)\n",
    "        \n",
    "    \n",
    "    return filtred_corpus,idx_doc\n",
    "\n",
    "# extracts topics with their terms and weights\n",
    "# format is Topic N: [(term1, weight1), ..., (termn, weightn)]        \n",
    "def get_topics_terms_weights(weights, feature_names):\n",
    "    feature_names = np.array(feature_names)\n",
    "    \n",
    "    #trie les indices des mots de chaque topics selon la poids du mots dans le topics\n",
    "    sorted_indices = np.array([list(row[::-1]) for row in np.argsort(np.abs(weights))])\n",
    "    \n",
    "    #trie les poids des mots de chaques topics,en recuperant les poids des indices deja triée\n",
    "    sorted_weights = np.array([list(wt[index]) for wt, index in zip(weights,sorted_indices)])\n",
    "    \n",
    "    #recupres les mots selon leurs indices deja triée\n",
    "    sorted_terms = np.array([list(feature_names[row]) for row in sorted_indices])\n",
    "    \n",
    "    #concatene chaque mots et sa poids sous formes de tuple (mot,poids)\n",
    "    topics = [np.vstack((terms.T, term_weights.T)).T for terms, term_weights in zip(sorted_terms, sorted_weights)]     \n",
    "    \n",
    "    return topics\n",
    "\n",
    "def compute_coherence_values(tfidf_train_features,feature_names,corpus,data_lemmatized,id2word,max_term=20,limit=50, start=5, step=5):\n",
    "    \"\"\"\n",
    "    Calcul la coherence UMass pour different nombre de topic\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    tfidf_train_features : features tf-idf qu'on va utiliser pour entrainer chaque model\n",
    "    feature_names : ensemble des mots contenue dans la matrice tf-idf\n",
    "    corpus: corpus de base qui contients les documents sous forme de texte\n",
    "    max_term: nombre maximal de mots qu'on va prendre pour calculé la coherence de chaque topic\n",
    "    data_lemmatized: corpus sous forme de tokens\n",
    "    id2word:vocabulaire du corpus au format de gensim\n",
    "    max_term:le nombre de termes qu'on va prendre dans chaque topic pour calculer la Coherence\n",
    "    limit : Nombre maximal de topics qu'on va tester\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    best_model : le model qui contient le plus grande coherence\n",
    "    coherence_values : Valeurs des Cohérences correspondant au modèle avec le nombre respectif de sujets\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model_list = [] #listes qui va contenir les modeles tester\n",
    "    coherence_values = [] #liste qui contenir les coherences de chaque models\n",
    "    # Term Document Frequency\n",
    "    \n",
    "    common_corpus = [id2word.doc2bow(text) for text in data_lemmatized] #recupere la matrice bog of word du corpus sous le format de gensim\n",
    "\n",
    "    \n",
    "    #print(coherence)\n",
    "    for num_topics in range(start, limit, step):\n",
    "        \n",
    "        model=NMF(n_components=num_topics,random_state=42) #model MNF\n",
    "        model.fit(tfidf_train_features)\n",
    "        weights = model.components_ #recupere les poids\n",
    "        \n",
    "        model_list.append(model) #ajoute le model la liste des models utilisé\n",
    "        \n",
    "        \n",
    "        topics=get_topics_terms_weights(weights,feature_names)\n",
    "        \n",
    "        topic_terms=getTopicTerms(topics)#recupere les mot des de chaque topics\n",
    "        \n",
    "        topic_terms=[topics[:max_term] for topics in topic_terms] #recupere les  \"max_term\" termes avec les plus grandes poids\n",
    "        \n",
    "        #calcule du Coherence UMass\n",
    "        cm = CoherenceModel(topics=topic_terms,corpus=common_corpus, dictionary=id2word, coherence='u_mass')\n",
    "        coherence = cm.get_coherence()\n",
    "        coherence_values.append(coherence)\n",
    "    \n",
    "    idx_max=np.array(coherence_values).argmax() #recupere l'indice du model qui possede le plus grands coherence\n",
    "    best_model=model_list[idx_max] #recupere le meilleur models\n",
    "    \n",
    "\n",
    "    return best_model,coherence_values\n",
    "\n",
    "def getTopicTerms(pos_topics):\n",
    "    \"\"\"\n",
    "    Fonction qui retourne l'ensemble des mots qui compose chaque topics\n",
    "    ----Input----\n",
    "    pos_topics: ensemble des topics qui contients les mots et leurs poids\n",
    "    ---output---\n",
    "    topic_terms : ensemble des mots des topics\n",
    "    \n",
    "    \"\"\"\n",
    "    topic_terms=[]\n",
    "    for topic in pos_topics:\n",
    "        #topic=topic[:max_term] #recupere les \"max_term\" premiere mots et leurs poids\n",
    "        terms=[]\n",
    "        for doc in topic:\n",
    "            terms.append(doc[0]) #recupere justes les mots sans les poids\n",
    "        \n",
    "        topic_terms.append(terms) #ajoute l'ensemble des mots\n",
    "    \n",
    "    return topic_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c806910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    \n",
    "    corpus=remove_characters(corpus)\n",
    "    corpus=lower_text(corpus)\n",
    "    corpus=remove_stopwords(corpus)\n",
    "    corpus=lemm_tokens(corpus)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145a7873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Confédération générale des entreprises du M...</td>\n",
       "      <td>2022-06-09</td>\n",
       "      <td>https://www.challenge.ma/maroc-emirats-arabes-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selon Bank Al-Maghrib, l’encours du crédit ban...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/banques-les-prets-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dans le cadre des Rencontres du Livre Blanc, l...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/la-cgem-met-le-cap-su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pour la première fois depuis la création des I...</td>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>https://www.challenge.ma/le-maroc-pays-africai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les levées des capitaux se sont établies à plu...</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>https://www.challenge.ma/marche-des-capitaux-l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       date  \\\n",
       "0  La Confédération générale des entreprises du M... 2022-06-09   \n",
       "1  Selon Bank Al-Maghrib, l’encours du crédit ban... 2022-06-03   \n",
       "2  Dans le cadre des Rencontres du Livre Blanc, l... 2022-06-03   \n",
       "3  Pour la première fois depuis la création des I... 2022-06-02   \n",
       "4  Les levées des capitaux se sont établies à plu... 2022-06-01   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.challenge.ma/maroc-emirats-arabes-...  \n",
       "1  https://www.challenge.ma/banques-les-prets-acc...  \n",
       "2  https://www.challenge.ma/la-cgem-met-le-cap-su...  \n",
       "3  https://www.challenge.ma/le-maroc-pays-africai...  \n",
       "4  https://www.challenge.ma/marche-des-capitaux-l...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(r\"..\\Scraping\\Raw_data\\1.csv\", sep=\",\", parse_dates=['date'])\n",
    "df2 = pd.read_csv(r\"..\\Scraping\\Raw_data\\2.csv\", sep=\",\", parse_dates=['date'])\n",
    "df3 = pd.read_csv(r\"..\\Scraping\\Raw_data\\3.csv\", sep=\",\", parse_dates=['date'])\n",
    "dataset = pd.concat([df1, df2, df3])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5314702",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['period']=dataset['date'].dt.to_period('M')\n",
    "dataset.sort_values(by=\"date\" ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c4b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du corpus = 74 Documents\n"
     ]
    }
   ],
   "source": [
    "corpus = dataset.text.values.tolist()\n",
    "print(\"Taille du corpus = \"+str(len(corpus))+\" Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2204c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocessing(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2880ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpuses_NMF (dataset):\n",
    "    corpus = dataset.text.values.tolist()\n",
    "    corpuses_NMF = []\n",
    "    l_periods=list(set(dataset.period.values))\n",
    "    l_periods.sort()\n",
    "    for i in range(len(l_periods)-1) :\n",
    "        indice1=dataset.period.values.tolist().index(l_periods[i])\n",
    "        indice2=dataset.period.values.tolist().index(l_periods[i+1])\n",
    "        liste = []\n",
    "        for j in range(indice1,indice2,1):\n",
    "            liste.append(corpus[j]) \n",
    "        corpuses_NMF.append(liste)\n",
    "\n",
    "    dernier_indice=dataset.period.values.tolist().index(l_periods[1])\n",
    "    derniere_corpus = [] \n",
    "    for j in range(dernier_indice,len(dataset),1):\n",
    "        derniere_corpus.append(corpus[j]) \n",
    "    corpuses_NMF.append(derniere_corpus)\n",
    "    return corpuses_NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b4a0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpuses_NMF = corpuses_NMF(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff190cd",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "178a2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build TFIDF features on train reviews with a specifique vocabulary\n",
    "corpus_lemmatized=tokenize_text(corpus) \n",
    "id2word = corpora.Dictionary(corpus_lemmatized)\n",
    "vocabulaire=id2word.token2id #get vocabulary dict where keys are terms and values are indices in the feature matrix\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0,sublinear_tf=True,lowercase=True,ngram_range=(1,2),vocabulary=vocabulaire)\n",
    "tfidf_train_features = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ae8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1133b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "tfidf_train_features_array = tfidf_train_features.toarray()\n",
    "tf_idf_frame = pd.DataFrame(tfidf_train_features_array,columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c50bb38",
   "metadata": {},
   "source": [
    "## bigrams processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3cfdf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bigrams = make_bigrams(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df544bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_frame_bigrams(list_bigrams):\n",
    "    \"\"\"Create DataFrame with list of bigrams to be used in association rule learning with R\"\"\"\n",
    "    \n",
    "    # Create a list of column names\n",
    "    columns_bigrams_df = []\n",
    "    for bigrams in list_bigrams :\n",
    "        for bigram in bigrams :\n",
    "            if bigram not in columns_bigrams_df :\n",
    "                columns_bigrams_df.append(bigram)\n",
    "    \n",
    "    # Create dataframe with bigrams\n",
    "    data_frame_bigrams = []\n",
    "    for bigrams in list_bigrams :\n",
    "        l_bigrams=[0 for i in range(len(columns_bigrams_df))]\n",
    "        for bigram in bigrams :\n",
    "            l_bigrams[columns_bigrams_df.index(bigram)]=1\n",
    "        data_frame_bigrams.append(l_bigrams)\n",
    "    \n",
    "    df = pd.DataFrame(data_frame_bigrams,columns=columns_bigrams_df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def  data_frame_tf_idf_bigrams(list_bigrams):\n",
    "    \"\"\"Create TF-IDF DataFrame with list of bigrams that contains the columns TF and IDF of each bigram\"\"\"\n",
    "    \n",
    "    # Create a list of column names\n",
    "    \n",
    "    unique_bigrams_columns = []\n",
    "    intermediate = []\n",
    "    list_tf_idf_bigrams = []\n",
    "    for bigrams in list_bigrams :\n",
    "        for bigram in bigrams :\n",
    "            if bigram not in unique_bigrams_columns :\n",
    "                unique_bigrams_columns.append(bigram)\n",
    "                intermediate.extend([bigram,bigram])\n",
    "                list_tf_idf_bigrams.extend([\"TF\",\"IDF\"])\n",
    "\n",
    "    # list of column names ==> 2 indexes : bigram and TF/IDF\n",
    "    columns_bigrams_df = [intermediate,list_tf_idf_bigrams] \n",
    "\n",
    "    index=[\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\n",
    "    \"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\",\"C27\",\"C28\",\"C29\",\"C30\",\"C31\",\"C32\",\n",
    "    \"C33\",\"C34\",\"C35\",\"C36\",\"C37\",\"C38\",\"C39\",\"C40\",\"C41\",\"C42\",\"C43\",\"C44\",\"C45\",\"C46\",\"C47\",\"C48\",\"C49\",\"C50\",\n",
    "    \"C51\",\"C52\",\"C53\",\"C54\",\"C55\",\"C56\",\"C57\",\"C58\",\"C59\",\"C60\",\"C61\",\"C62\",\"C63\",\"C64\",\"C65\",\"C66\",\"C67\",\"C68\",\"C69\",\n",
    "    \"C70\",\"C71\",\"C72\",\"C73\",\"C74\"]\n",
    "    \n",
    "    # Create tf-idf dataframe with bigrams\n",
    "    data_frame_bigrams = []\n",
    "\n",
    "    for bigrams in list_bigrams :\n",
    "        l_bigrams=[]\n",
    "        for bigram in unique_bigrams_columns :\n",
    "            l_bigrams.extend([tf_function(bigrams,bigram),idf_function(list_bigrams,bigram)])\n",
    "        data_frame_bigrams.append(l_bigrams)\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame(data_frame_bigrams,columns=columns_bigrams_df,index=index)\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2229561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigrams = data_frame_bigrams(list_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f40c6c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_bigrams = data_frame_tf_idf_bigrams(list_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0452aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ define process function ########################## \n",
    "\n",
    "def process_corpus(corpus,indice):\n",
    "    \"\"\"retourne une liste output\"\"\"\n",
    "    ########### former la matrice tf-idf #############################\n",
    "    corpus_lemmatized=tokenize_text(corpus) \n",
    "    id2word = corpora.Dictionary(corpus_lemmatized)\n",
    "    vocabulaire=id2word.token2id \n",
    "    tfidf = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0,sublinear_tf=True,\n",
    "                        lowercase=True,ngram_range=(1,2),vocabulary=vocabulaire)\n",
    "    tfidf_train_features = tfidf.fit_transform(corpus)\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    ################# Topic Modelling : NMF ############################################\n",
    "    total_topics = 10\n",
    "    pos_nmf=NMF(n_components=total_topics,random_state=42,l1_ratio=0.2,max_iter=200)\n",
    "    pos_nmf.fit(tfidf_train_features) \n",
    "    pos_weights = pos_nmf.components_\n",
    "    pos_topics = get_topics_terms_weights(pos_weights, feature_names)\n",
    "    topic_terms=getTopicTerms(pos_topics)\n",
    "    topic_terms=[topics[:20] for topics in topic_terms] \n",
    "    common_corpus = [id2word.doc2bow(text) for text in corpus_lemmatized] \n",
    "    ####### choix du meilleur model ###################################\n",
    "    cm = CoherenceModel(topics=topic_terms,corpus=common_corpus, dictionary=id2word, \n",
    "               coherence='u_mass')\n",
    "    coherence = cm.get_coherence()\n",
    "    best_model,coherence_values=compute_coherence_values(tfidf_train_features,\n",
    "    feature_names,corpus,corpus_lemmatized,id2word,max_term=20,limit=50)\n",
    "    ######## résultat du modèle optimal ##############################\n",
    "    total_topics=best_model.n_components \n",
    "    weights = best_model.components_ \n",
    "    topics = get_topics_terms_weights(weights,feature_names)\n",
    "    ##### Data_For_Plot ########################################################\n",
    "    period = l_periods[indice]\n",
    "    dataset_modified = dataset.loc[dataset.period == period , :]\n",
    "    dates=pd.to_datetime(dataset_modified.loc[:,\"date\"].values)\n",
    "    doc_topic_dist = best_model.transform(tfidf_train_features) \n",
    "    labels=getTopicTerms(topics)\n",
    "    labels=[\",\".join(topic_term[:5]) for topic_term in labels] \n",
    "    df=pd.DataFrame({\"text\":corpus,\"Date\":dates,\"doc_num\":np.arange(len(corpus))})\n",
    "    stories=df.groupby(\"doc_num\")[\"text\",\"Date\"].min().reset_index() \n",
    "    story_topics_for_plot=pd.DataFrame(dict(doc_num=np.arange(doc_topic_dist.shape[0])))\n",
    "    for idx in range(len(labels)):\n",
    "        story_topics_for_plot[labels[idx]] = doc_topic_dist[:, idx]\n",
    "    trends = stories.merge(story_topics_for_plot, on='doc_num')\n",
    "    mass = lambda x: ((x) * 1.0).sum() / x.shape[0]  \n",
    "    window = 10\n",
    "    trend_indice = min(len(labels),5)\n",
    "    aggs = {labels[i]: mass for i in range(trend_indice) }\n",
    "    data_for_plot=trends.groupby(trends['Date'].dt.date).agg(aggs).rolling(window).mean()\n",
    "\n",
    "    ######### output de chaque corpus ###############################\n",
    "    output=[story_topics_for_plot,data_for_plot]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48e0faa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "C:\\Users\\ORIGIN~1\\AppData\\Local\\Temp/ipykernel_2748/3932622729.py:40: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  stories=df.groupby(\"doc_num\")[\"text\",\"Date\"].min().reset_index()\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "c:\\Users\\Original Shop\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "C:\\Users\\ORIGIN~1\\AppData\\Local\\Temp/ipykernel_2748/3932622729.py:40: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  stories=df.groupby(\"doc_num\")[\"text\",\"Date\"].min().reset_index()\n"
     ]
    }
   ],
   "source": [
    "outputs =[]\n",
    "for i in range(len(corpuses_NMF)): \n",
    "    outputs.append(process_corpus(corpuses_NMF[i],i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9b075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe697844cfe8a1028ac8cd95af0dad884ddcd4494e6159c72486af12df7aa875"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
