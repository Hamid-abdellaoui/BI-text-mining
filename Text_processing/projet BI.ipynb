{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee645fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des bibiotheque necessaire\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "import re\n",
    "#from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import fr_core_news_md #import spacy french stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e3fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "def tokenize_text(corpus):\n",
    "    tokensCorpus=[]\n",
    "    for doc in corpus:\n",
    "        doc_tokens = word_tokenize(doc)\n",
    "        tokensCorpus.append(doc_tokens)\n",
    "    return tokensCorpus\n",
    "\n",
    "# removing stopwords\n",
    "def remove_stopwords(corpus):\n",
    "    filtered_corpus=[]\n",
    "    for tokens in corpus:\n",
    "        #french_sw = stopwords.words('french') \n",
    "        french_sw=list(STOP_WORDS) #get french stopwords\n",
    "        filtered_tokens = [token for token in tokens.split() if token not in french_sw and len(token)>2]\n",
    "        filtred_text=' '.join(filtered_tokens) #reforme le text du documents separé par espace\n",
    "        filtered_corpus.append(filtred_text)\n",
    "    return filtered_corpus\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    list_bigrams=[]\n",
    "    for doc in texts :\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(doc)\n",
    "        # bigrams \n",
    "        bigrams = [w for w in ngrams(tokens,n=2)]\n",
    "        list_bigrams.append(bigrams)\n",
    "    return list_bigrams\n",
    "\n",
    "def unique_bigrams(texts):\n",
    "    list_bigrams = make_bigrams(texts)\n",
    "    list_unique_bigrams=[]\n",
    "    for list_bigram in list_bigrams :\n",
    "        list_unique_bigrams.append(list(set(list_bigram)))\n",
    "    return list_unique_bigrams\n",
    "\n",
    "def count_unique_bigrams(texts):\n",
    "    list_unique_bigrams = unique_bigrams(texts)\n",
    "    list_count_unique_bigrams = []\n",
    "    for list_unique_bigram in list_unique_bigrams :\n",
    "        list_count_unique_bigrams.append(len(list_unique_bigram))\n",
    "    return list_count_unique_bigrams\n",
    "\n",
    "def tf_function(bigrams,bigram):\n",
    "    n=len(bigrams) #nombre de bigrams uniques dans le texte\n",
    "    m= bigrams.count(bigram) # nombre d'occurence du bigram en parametre dans la liste de bigrams\n",
    "    return  m/(n+1)\n",
    "\n",
    "def idf_function(whole_bigram,bigram):\n",
    "    list_idf = []\n",
    "    s = 0\n",
    "    for bigrams in whole_bigram:  \n",
    "        if bigram in bigrams:\n",
    "            s += 1\n",
    "    return np.log((len(whole_bigram)/(s+1))+1)\n",
    "\n",
    "def tf_idf_function(whole_bigram,bigrams,bigram):\n",
    "    return tf_function(bigrams,bigram) * idf_function(whole_bigram,bigram)\n",
    "\n",
    "#output French accents correctly\n",
    "def convert_accents(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore')\n",
    "\n",
    "#convertisse les documents en minuscule\n",
    "def lower_text(corpus):\n",
    "    LowerCorpus=[]\n",
    "    for doc in corpus:\n",
    "        lowerDoc=str(doc).lower() #convertissent le texte en minuscules\n",
    "        lowerDoc=convert_accents(lowerDoc).decode(\"utf-8\") #supprimes les accents\n",
    "        LowerCorpus.append(lowerDoc)\n",
    "    return LowerCorpus\n",
    "\n",
    "#supprimes caracteres speciaux\n",
    "def remove_characters(corpus,keep_apostrophes=True):\n",
    "    filtered_corpus=[]\n",
    "    for doc in corpus:\n",
    "        doc = doc.strip()\n",
    "        if keep_apostrophes:\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            doc =re.sub(\"l\\'\",\"\",doc)\n",
    "            doc =re.sub(\"d\\'\",\"\",doc)\n",
    "            PATTERN = r'[?|$|&|*|-|!|%|@|(|)|~|\\d]'\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "            filtered_corpus.append(filtered_doc)\n",
    "        else:\n",
    "            PATTERN = r'[^a-zA-Z ]'\n",
    "            #supprimes les urls\n",
    "            doc =re.sub('(https|http)\\S*\\s?', '',doc) #supprimes les urls\n",
    "            filtered_doc = re.sub(PATTERN, r'', doc)\n",
    "        \n",
    "            filtered_corpus.append(filtered_doc)\n",
    "    return filtered_corpus\n",
    "\n",
    "#recuperer les mots qui apparaissent dans plusieurs documents\n",
    "def get_mostCommonWords(corpus,max_freq=100):\n",
    "    vocabulaire=dict() #dictionnaire qui va contenir le nombre d'occurence des mots dans les documents\n",
    "    for doc in corpus:\n",
    "        for word in set(doc.split()): #recupere les mots unique de chaque documents\n",
    "            if word in vocabulaire:\n",
    "                vocabulaire[word]+=1\n",
    "            else:\n",
    "                vocabulaire[word]=1\n",
    "    \n",
    "    #recupere les dont le nombre d'occurences dans les documents > max_freq\n",
    "    mostCommonsWord=[word for word,value in vocabulaire.items() if value>max_freq ]\n",
    "        \n",
    "    return mostCommonsWord\n",
    "\n",
    "#lemmatisation\n",
    "def lemm_tokens(corpus):\n",
    "    \n",
    "    nlp = fr_core_news_md.load() #initialisation du model \"fr_core_news_md\" de spacy\n",
    "    allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    corpus_lemms=[]\n",
    "    \n",
    "    for document in corpus:\n",
    "        doc = nlp(document)\n",
    "        lemms=[token.lemma_ for token in doc if token.pos_ in allowed_postags] #recupere les lemms des tokens\n",
    "        text=' '.join(lemms) #reforme le text du documents separé par espace\n",
    "        corpus_lemms.append(text)\n",
    "            \n",
    "    return corpus_lemms\n",
    "#fonction qui supprimes les documents vides ou tres courte\n",
    "def remove_shortDocument(corpus,min_length=3):\n",
    "    filtred_corpus=[]\n",
    "    idx_doc=[]\n",
    "    for idx,doc in enumerate(corpus):\n",
    "        \n",
    "        if len(doc.split())>min_length:\n",
    "            filtred_corpus.append(doc)\n",
    "            idx_doc.append(idx)\n",
    "        \n",
    "    \n",
    "    return filtred_corpus,idx_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c806910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    \n",
    "    corpus=remove_characters(corpus)\n",
    "    corpus=lower_text(corpus)\n",
    "    corpus=remove_stopwords(corpus)\n",
    "    corpus=lemm_tokens(corpus)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145a7873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Confédération générale des entreprises du M...</td>\n",
       "      <td>2022-06-09</td>\n",
       "      <td>https://www.challenge.ma/maroc-emirats-arabes-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selon Bank Al-Maghrib, l’encours du crédit ban...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/banques-les-prets-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dans le cadre des Rencontres du Livre Blanc, l...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/la-cgem-met-le-cap-su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pour la première fois depuis la création des I...</td>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>https://www.challenge.ma/le-maroc-pays-africai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les levées des capitaux se sont établies à plu...</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>https://www.challenge.ma/marche-des-capitaux-l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        date  \\\n",
       "0  La Confédération générale des entreprises du M...  2022-06-09   \n",
       "1  Selon Bank Al-Maghrib, l’encours du crédit ban...  2022-06-03   \n",
       "2  Dans le cadre des Rencontres du Livre Blanc, l...  2022-06-03   \n",
       "3  Pour la première fois depuis la création des I...  2022-06-02   \n",
       "4  Les levées des capitaux se sont établies à plu...  2022-06-01   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.challenge.ma/maroc-emirats-arabes-...  \n",
       "1  https://www.challenge.ma/banques-les-prets-acc...  \n",
       "2  https://www.challenge.ma/la-cgem-met-le-cap-su...  \n",
       "3  https://www.challenge.ma/le-maroc-pays-africai...  \n",
       "4  https://www.challenge.ma/marche-des-capitaux-l...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(r\"..\\Scraping\\Raw_data\\1.csv\", sep=\",\")\n",
    "df2 = pd.read_csv(r\"..\\Scraping\\Raw_data\\2.csv\", sep=\",\")\n",
    "df3 = pd.read_csv(r\"..\\Scraping\\Raw_data\\3.csv\", sep=\",\")\n",
    "dataset = pd.concat([df1, df2, df3])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5314702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Confédération générale des entreprises du M...</td>\n",
       "      <td>2022-06-09</td>\n",
       "      <td>https://www.challenge.ma/maroc-emirats-arabes-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selon Bank Al-Maghrib, l’encours du crédit ban...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/banques-les-prets-acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dans le cadre des Rencontres du Livre Blanc, l...</td>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>https://www.challenge.ma/la-cgem-met-le-cap-su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pour la première fois depuis la création des I...</td>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>https://www.challenge.ma/le-maroc-pays-africai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les levées des capitaux se sont établies à plu...</td>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>https://www.challenge.ma/marche-des-capitaux-l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        date  \\\n",
       "0  La Confédération générale des entreprises du M...  2022-06-09   \n",
       "1  Selon Bank Al-Maghrib, l’encours du crédit ban...  2022-06-03   \n",
       "2  Dans le cadre des Rencontres du Livre Blanc, l...  2022-06-03   \n",
       "3  Pour la première fois depuis la création des I...  2022-06-02   \n",
       "4  Les levées des capitaux se sont établies à plu...  2022-06-01   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.challenge.ma/maroc-emirats-arabes-...  \n",
       "1  https://www.challenge.ma/banques-les-prets-acc...  \n",
       "2  https://www.challenge.ma/la-cgem-met-le-cap-su...  \n",
       "3  https://www.challenge.ma/le-maroc-pays-africai...  \n",
       "4  https://www.challenge.ma/marche-des-capitaux-l...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b79dad11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame index must be unique for orient='columns'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hp\\Desktop\\BI-text-mining\\Text_processing\\projet BI.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/BI-text-mining/Text_processing/projet%20BI.ipynb#ch0000019?line=0'>1</a>\u001b[0m json \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mto_json(orient\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/BI-text-mining/Text_processing/projet%20BI.ipynb#ch0000019?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(json)\n",
      "File \u001b[1;32mc:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\core\\generic.py:2633\u001b[0m, in \u001b[0;36mNDFrame.to_json\u001b[1;34m(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options)\u001b[0m\n\u001b[0;32m   2630\u001b[0m config\u001b[39m.\u001b[39mis_nonnegative_int(indent)\n\u001b[0;32m   2631\u001b[0m indent \u001b[39m=\u001b[39m indent \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 2633\u001b[0m \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39;49mto_json(\n\u001b[0;32m   2634\u001b[0m     path_or_buf\u001b[39m=\u001b[39;49mpath_or_buf,\n\u001b[0;32m   2635\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2636\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[0;32m   2637\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   2638\u001b[0m     double_precision\u001b[39m=\u001b[39;49mdouble_precision,\n\u001b[0;32m   2639\u001b[0m     force_ascii\u001b[39m=\u001b[39;49mforce_ascii,\n\u001b[0;32m   2640\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[0;32m   2641\u001b[0m     default_handler\u001b[39m=\u001b[39;49mdefault_handler,\n\u001b[0;32m   2642\u001b[0m     lines\u001b[39m=\u001b[39;49mlines,\n\u001b[0;32m   2643\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   2644\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   2645\u001b[0m     indent\u001b[39m=\u001b[39;49mindent,\n\u001b[0;32m   2646\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   2647\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\io\\json\\_json.py:110\u001b[0m, in \u001b[0;36mto_json\u001b[1;34m(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobj\u001b[39m\u001b[39m'\u001b[39m\u001b[39m should be a Series or a DataFrame\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 110\u001b[0m s \u001b[39m=\u001b[39m writer(\n\u001b[0;32m    111\u001b[0m     obj,\n\u001b[0;32m    112\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[0;32m    113\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m    114\u001b[0m     double_precision\u001b[39m=\u001b[39;49mdouble_precision,\n\u001b[0;32m    115\u001b[0m     ensure_ascii\u001b[39m=\u001b[39;49mforce_ascii,\n\u001b[0;32m    116\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[0;32m    117\u001b[0m     default_handler\u001b[39m=\u001b[39;49mdefault_handler,\n\u001b[0;32m    118\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m    119\u001b[0m     indent\u001b[39m=\u001b[39;49mindent,\n\u001b[0;32m    120\u001b[0m )\u001b[39m.\u001b[39mwrite()\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m lines:\n\u001b[0;32m    123\u001b[0m     s \u001b[39m=\u001b[39m convert_to_line_delimits(s)\n",
      "File \u001b[1;32mc:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\io\\json\\_json.py:165\u001b[0m, in \u001b[0;36mWriter.__init__\u001b[1;34m(self, obj, orient, date_format, double_precision, ensure_ascii, date_unit, index, default_handler, indent)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent \u001b[39m=\u001b[39m indent\n\u001b[0;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_copy \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format_axes()\n",
      "File \u001b[1;32mc:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\io\\json\\_json.py:222\u001b[0m, in \u001b[0;36mFrameWriter._format_axes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[39mTry to format axes if they are datelike.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mis_unique \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 222\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    223\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataFrame index must be unique for orient=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m     )\n\u001b[0;32m    225\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient \u001b[39min\u001b[39;00m (\n\u001b[0;32m    226\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    228\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    229\u001b[0m ):\n\u001b[0;32m    230\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    231\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataFrame columns must be unique for orient=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame index must be unique for orient='columns'."
     ]
    }
   ],
   "source": [
    "json = dataset.to_json(orient=\"index\")\n",
    "print(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c4b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du corpus = 74 Documents\n"
     ]
    }
   ],
   "source": [
    "corpus = dataset.text.values.tolist()\n",
    "print(\"Taille du corpus = \"+str(len(corpus))+\" Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24aca84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocessing(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff190cd",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "178a2bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1322: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# build TFIDF features on train reviews with a specifique vocabulary\n",
    "corpus_lemmatized=tokenize_text(corpus) \n",
    "id2word = corpora.Dictionary(corpus_lemmatized)\n",
    "vocabulaire=id2word.token2id #get vocabulary dict where keys are terms and values are indices in the feature matrix\n",
    "\n",
    "tfidf = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0,sublinear_tf=True,lowercase=True,ngram_range=(1,2),vocabulary=vocabulaire)\n",
    "tfidf_train_features = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90ae8aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1133b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "tfidf_train_features_array = tfidf_train_features.toarray()\n",
    "tf_idf_frame = pd.DataFrame(tfidf_train_features_array,columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa68419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-</th>\n",
       "      <th>abdelmajid</th>\n",
       "      <th>abdullah</th>\n",
       "      <th>actuel</th>\n",
       "      <th>affaire</th>\n",
       "      <th>affirmer</th>\n",
       "      <th>ajoutee</th>\n",
       "      <th>ajouter</th>\n",
       "      <th>albare</th>\n",
       "      <th>algerienn</th>\n",
       "      <th>...</th>\n",
       "      <th>migrate</th>\n",
       "      <th>outsourcia</th>\n",
       "      <th>reagissant</th>\n",
       "      <th>relais</th>\n",
       "      <th>sp</th>\n",
       "      <th>spe</th>\n",
       "      <th>sujette</th>\n",
       "      <th>suspensif</th>\n",
       "      <th>trajectoire</th>\n",
       "      <th>usuel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068419</td>\n",
       "      <td>0.115843</td>\n",
       "      <td>0.039628</td>\n",
       "      <td>0.052164</td>\n",
       "      <td>0.054862</td>\n",
       "      <td>0.046165</td>\n",
       "      <td>0.041909</td>\n",
       "      <td>0.068419</td>\n",
       "      <td>0.068419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     -  abdelmajid  abdullah    actuel   affaire  affirmer   ajoutee  \\\n",
       "0  0.0    0.068419  0.115843  0.039628  0.052164  0.054862  0.046165   \n",
       "1  0.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    ajouter    albare  algerienn  ...  migrate  outsourcia  reagissant  \\\n",
       "0  0.041909  0.068419   0.068419  ...      0.0         0.0         0.0   \n",
       "1  0.000000  0.000000   0.000000  ...      0.0         0.0         0.0   \n",
       "2  0.000000  0.000000   0.000000  ...      0.0         0.0         0.0   \n",
       "3  0.000000  0.000000   0.000000  ...      0.0         0.0         0.0   \n",
       "4  0.000000  0.000000   0.000000  ...      0.0         0.0         0.0   \n",
       "\n",
       "   relais   sp  spe  sujette  suspensif  trajectoire  usuel  \n",
       "0     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "1     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "2     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "3     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "4     0.0  0.0  0.0      0.0        0.0          0.0    0.0  \n",
       "\n",
       "[5 rows x 4753 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c50bb38",
   "metadata": {},
   "source": [
    "## bigrams processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3cfdf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bigrams = make_bigrams(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc880138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[]\n",
    "for bigrams in list_bigrams :\n",
    "    l.append(len(bigrams))\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df544bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_frame_bigrams(list_bigrams):\n",
    "    \"\"\"Create DataFrame with list of bigrams to be used in association rule learning with R\"\"\"\n",
    "    \n",
    "    # Create a list of column names\n",
    "    columns_bigrams_df = []\n",
    "    for bigrams in list_bigrams :\n",
    "        for bigram in bigrams :\n",
    "            if bigram not in columns_bigrams_df :\n",
    "                columns_bigrams_df.append(bigram)\n",
    "    \n",
    "    # Create dataframe with bigrams\n",
    "    data_frame_bigrams = []\n",
    "    for bigrams in list_bigrams :\n",
    "        l_bigrams=[0 for i in range(len(columns_bigrams_df))]\n",
    "        for bigram in bigrams :\n",
    "            l_bigrams[columns_bigrams_df.index(bigram)]=1\n",
    "        data_frame_bigrams.append(l_bigrams)\n",
    "    \n",
    "    df = pd.DataFrame(data_frame_bigrams,columns=columns_bigrams_df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def  data_frame_tf_idf_bigrams(list_bigrams):\n",
    "    \"\"\"Create TF-IDF DataFrame with list of bigrams that contains the columns TF and IDF of each bigram\"\"\"\n",
    "    \n",
    "    # Create a list of column names\n",
    "    \n",
    "    unique_bigrams_columns = []\n",
    "    intermediate = []\n",
    "    list_tf_idf_bigrams = []\n",
    "    for bigrams in list_bigrams :\n",
    "        for bigram in bigrams :\n",
    "            if bigram not in unique_bigrams_columns :\n",
    "                unique_bigrams_columns.extend(bigram)\n",
    "                intermediate.extend([bigram,bigram])\n",
    "                list_tf_idf_bigrams.extend([\"TF\",\"IDF\"])\n",
    "\n",
    "    # list of column names ==> 2 indexes : bigram and TF/IDF\n",
    "    columns_bigrams_df = [intermediate,list_tf_idf_bigrams] \n",
    "\n",
    "    index=[\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\n",
    "    \"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\",\"C27\",\"C28\",\"C29\",\"C30\",\"C31\",\"C32\",\n",
    "    \"C33\",\"C34\",\"C35\",\"C36\",\"C37\",\"C38\",\"C39\",\"C40\",\"C41\",\"C42\",\"C43\",\"C44\",\"C45\",\"C46\",\"C47\",\"C48\",\"C49\",\"C50\",\n",
    "    \"C51\",\"C52\",\"C53\",\"C54\",\"C55\",\"C56\",\"C57\",\"C58\",\"C59\",\"C60\",\"C61\",\"C62\",\"C63\",\"C64\",\"C65\",\"C66\",\"C67\",\"C68\",\"C69\",\n",
    "    \"C70\",\"C71\",\"C72\",\"C73\",\"C74\"]\n",
    "    \n",
    "    # Create tf-idf dataframe with bigrams\n",
    "    data_frame_bigrams = []\n",
    "\n",
    "    for bigrams in list_bigrams :\n",
    "        l_bigrams=[]\n",
    "        for bigram in unique_bigrams_columns :\n",
    "            l_bigrams.extend([tf_function(bigrams,bigram),idf_function(list_bigrams,bigram)])\n",
    "        data_frame_bigrams.append(l_bigrams)\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame(data_frame_bigrams,columns=columns_bigrams_df,index=index)\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2229561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigrams = data_frame_bigrams(list_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f40c6c43",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "39266 columns passed, passed data had 78532 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:982\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 982\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    983\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    984\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:1030\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_mi_list \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(columns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(content):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m     \u001b[39m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m-> 1030\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m   1031\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(columns)\u001b[39m}\u001b[39;00m\u001b[39m columns passed, passed data had \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1032\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(content)\u001b[39m}\u001b[39;00m\u001b[39m columns\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m     )\n\u001b[0;32m   1034\u001b[0m \u001b[39melif\u001b[39;00m is_mi_list:\n\u001b[0;32m   1035\u001b[0m \n\u001b[0;32m   1036\u001b[0m     \u001b[39m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 39266 columns passed, passed data had 78532 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hp\\Desktop\\BI-text-mining\\Text_processing\\projet BI.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/BI-text-mining/Text_processing/projet%20BI.ipynb#ch0000017?line=0'>1</a>\u001b[0m tf_idf_bigrams \u001b[39m=\u001b[39m data_frame_tf_idf_bigrams(list_bigrams)\n",
      "\u001b[1;32mc:\\Users\\hp\\Desktop\\BI-text-mining\\Text_processing\\projet BI.ipynb Cell 16'\u001b[0m in \u001b[0;36mdata_frame_tf_idf_bigrams\u001b[1;34m(list_bigrams)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/BI-text-mining/Text_processing/projet%20BI.ipynb#ch0000015?line=52'>53</a>\u001b[0m         l_bigrams\u001b[39m.\u001b[39mextend([tf_function(bigrams,bigram),idf_function(list_bigrams,bigram)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/BI-text-mining/Text_processing/projet%20BI.ipynb#ch0000015?line=53'>54</a>\u001b[0m     data_frame_bigrams\u001b[39m.\u001b[39mappend(l_bigrams)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/BI-text-mining/Text_processing/projet%20BI.ipynb#ch0000015?line=56'>57</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(data_frame_bigrams,columns\u001b[39m=\u001b[39;49mcolumns_bigrams_df,index\u001b[39m=\u001b[39;49mindex)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/BI-text-mining/Text_processing/projet%20BI.ipynb#ch0000015?line=58'>59</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\core\\frame.py:721\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m         \u001b[39m# error: Argument 1 to \"ensure_index\" has incompatible type\u001b[39;00m\n\u001b[0;32m    718\u001b[0m         \u001b[39m# \"Collection[Any]\"; expected \"Union[Union[Union[ExtensionArray,\u001b[39;00m\n\u001b[0;32m    719\u001b[0m         \u001b[39m# ndarray], Index, Series], Sequence[Any]]\"\u001b[39;00m\n\u001b[0;32m    720\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    722\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    723\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    724\u001b[0m         data,\n\u001b[0;32m    725\u001b[0m         columns,\n\u001b[0;32m    726\u001b[0m         index,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    727\u001b[0m         dtype,\n\u001b[0;32m    728\u001b[0m     )\n\u001b[0;32m    729\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    730\u001b[0m         arrays,\n\u001b[0;32m    731\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    734\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    735\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:519\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields)\n\u001b[1;32m--> 519\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    520\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    522\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:883\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    880\u001b[0m     data \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[0;32m    881\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 883\u001b[0m content, columns \u001b[39m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    884\u001b[0m \u001b[39mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32mc:\\Users\\hp\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:985\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    982\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    983\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    984\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 985\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(contents) \u001b[39mand\u001b[39;00m contents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[0;32m    988\u001b[0m     contents \u001b[39m=\u001b[39m _convert_object_array(contents, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 39266 columns passed, passed data had 78532 columns"
     ]
    }
   ],
   "source": [
    "tf_idf_bigrams = data_frame_tf_idf_bigrams(list_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0713e1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">a</th>\n",
       "      <th colspan=\"2\" halign=\"left\">b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>TF</th>\n",
       "      <th>IDF</th>\n",
       "      <th>TF</th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data1</th>\n",
       "      <td>0.180393</td>\n",
       "      <td>0.343482</td>\n",
       "      <td>0.441857</td>\n",
       "      <td>0.616847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data2</th>\n",
       "      <td>0.381456</td>\n",
       "      <td>0.794810</td>\n",
       "      <td>0.965859</td>\n",
       "      <td>0.682141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              a                   b          \n",
       "             TF       IDF        TF       IDF\n",
       "data1  0.180393  0.343482  0.441857  0.616847\n",
       "data2  0.381456  0.794810  0.965859  0.682141"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = ['a', 'a', 'b', 'b']\n",
    "l2 = [\"TF\", \"IDF\", \"TF\", \"IDF\"]\n",
    "l=[l1,l2]\n",
    "pd.DataFrame(np.random.rand(2, 4),index=[\"data1\" , \"data2\"],columns=l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "19084cb2a221719542bd94e588e69917ee95725b301fa5f2df470f319dde6836"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
